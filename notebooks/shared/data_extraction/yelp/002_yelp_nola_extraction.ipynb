{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71ff581f",
   "metadata": {},
   "source": [
    "# Tourism Sentiment Analysis - Yelp New Orleans Data Extraction\n",
    "\n",
    "**Project:** Tourism Sentiment Analysis\n",
    "\n",
    "**Task:** Data Extraction & Processing\n",
    "\n",
    "**Dataset Source:** Yelp Academic Dataset (Kaggle)\n",
    "\n",
    "**Focus:** New Orleans, 2013/2016/2018, Tourism Businesses\n",
    "\n",
    "**Source URL:** https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset\n",
    "\n",
    "---\n",
    "<br>\n",
    "<details>\n",
    "<summary><strong>Dataset Update Warning</strong> (click to expand)</summary>\n",
    "\n",
    "Yelp Academic Dataset is updated annually with newer data and potential schema changes.\n",
    "\n",
    "Current dataset covers 2005-2022, but future versions may:\n",
    "- Remove older historical data (pre-2010)\n",
    "- Change JSON structure or column names\n",
    "- Modify business categorization system\n",
    "\n",
    "This notebook includes validation to detect significant dataset changes.\n",
    "</details>\n",
    "\n",
    "---\n",
    "<br>\n",
    "<details>\n",
    "<summary><strong>Historical Tourism Events Focus</strong> (click to expand)</summary>\n",
    "\n",
    "This analysis focuses on major tourism events in New Orleans to understand how volume fluctuations affect customer sentiment:\n",
    "\n",
    "- **2013:** Super Bowl XLVII (February) - massive tourism influx\n",
    "- **2016:** Normal tourism baseline - regular seasonal patterns  \n",
    "- **2018:** New Orleans Tricentennial - 300th anniversary celebrations\n",
    "\n",
    "The workflow filters to these specific years after 2012+ data quality threshold.\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cf1e14",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration\n",
    "*Import libraries, set up project paths, create directory structure*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7458fbfc",
   "metadata": {},
   "source": [
    "### 1A. Imports & Script Setup\n",
    "*Load required packages and configure script imports*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5393f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# File handling & JSON processing\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# System utilities\n",
    "import sys\n",
    "\n",
    "# Bootstrap: Add shared scripts to Python path\n",
    "def setup_scripts_path():\n",
    "    \"\"\"Add shared scripts to Python path\"\"\"\n",
    "    current = Path.cwd()\n",
    "    for _ in range(10):\n",
    "        if (current / '.projectroot').exists():\n",
    "            scripts_dir = current / 'notebooks' / 'shared' / 'scripts'\n",
    "            if scripts_dir.exists():\n",
    "                sys.path.insert(0, str(scripts_dir))\n",
    "                return scripts_dir\n",
    "        if current.parent == current:\n",
    "            break\n",
    "        current = current.parent\n",
    "    raise FileNotFoundError(\n",
    "        \"Scripts directory not found.\\n\"\n",
    "        \"Ensure .projectroot exists at project root.\"\n",
    "    )\n",
    "\n",
    "# Setup path and import utilities\n",
    "scripts_dir = setup_scripts_path()\n",
    "\n",
    "from project_utils import find_project_root\n",
    "from data_io import setup_extraction_directories, check_existing_file, check_existing_chunks\n",
    "from data_validation import print_final_summary, print_storage_summary\n",
    "\n",
    "# Define target files for download\n",
    "TARGET_FILES = [\n",
    "    'yelp_academic_dataset_business.json',\n",
    "    'yelp_academic_dataset_review.json',\n",
    "    'yelp_academic_dataset_user.json'\n",
    "]\n",
    "\n",
    "print(f\"✓ Packages and scripts loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf2559b",
   "metadata": {},
   "source": [
    "### 1B. Project Root Detection\n",
    "*Cross-platform function to locate project directory automatically*\n",
    "\n",
    "**Purpose:** Finds project root by searching for `.projectroot` marker file\n",
    "\n",
    "**Handles:** Working directory issues, different operating systems, various notebook locations\n",
    "\n",
    "**Confirmation:** Displays detected path for verification\n",
    "\n",
    "**Manual Override:** Uncomment line below if auto-detection fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b7cb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically detect project root\n",
    "project_root = find_project_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4543d2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If auto-detection fails, uncomment and edit this line:\n",
    "# project_root = Path('/.../.../.../[tourism_data_project]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a894316",
   "metadata": {},
   "source": [
    "### 1C. Set Project Paths\n",
    "*Establish standardized directory structure for bronze and silver processing*\n",
    "\n",
    "**Bronze Structure:** Raw download → chunked conversion → primary filter\n",
    "\n",
    "**Silver Structure:** Final staging area for gold layer integration\n",
    "\n",
    "**Auto-creation:** All directories created automatically for new collaborators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a83d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all required directories automatically\n",
    "dirs = setup_extraction_directories(project_root, 'yelp')\n",
    "\n",
    "# Access directories throughout notebook\n",
    "original_dir = dirs['bronze_original']\n",
    "conversion_dir = dirs['bronze_conversion']\n",
    "primary_filter_dir = dirs['bronze_primary_filter']\n",
    "silver_staging = dirs['silver_staging']\n",
    "\n",
    "print(\"Directories ready:\")\n",
    "print(f\"  Bronze original: {original_dir}\")\n",
    "print(f\"  Bronze conversion: {conversion_dir}\")\n",
    "print(f\"  Bronze primary filter: {primary_filter_dir}\")\n",
    "print(f\"  Silver staging: {silver_staging}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1866b370",
   "metadata": {},
   "source": [
    "## 2. Data Acquisition\n",
    "*Download raw JSON files from Kaggle using Yelp Academic Dataset*\n",
    "\n",
    "**Source:** Official Yelp Academic Dataset via Kaggle API\n",
    "\n",
    "**Files:** Business, Review, and User JSON files (~7GB total)\n",
    "\n",
    "**Authentication:** Requires Kaggle API credentials (setup below)\n",
    "\n",
    "***<u>Important Note</u>:*** If API setup fails or if manual download is preferable, *please skip to Section 2C*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f951f1",
   "metadata": {},
   "source": [
    "### 2A. Kaggle API Configuration\n",
    "*Step-by-step setup for Yelp Academic Dataset access*\n",
    "\n",
    "**Required:** Kaggle account and API credentials for 8GB dataset download\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a699cbc1",
   "metadata": {},
   "source": [
    "#### 2A.1 Kaggle Account & API Token Setup\n",
    "\n",
    "<details>\n",
    "<summary>Create account and generate credentials (click to expand)</summary>\n",
    "\n",
    "**Visit:** https://www.kaggle.com/\n",
    "\n",
    "**Register/Login:** Use email or Google account\n",
    "\n",
    "**Generate API Token:**\n",
    "   - Go to: https://www.kaggle.com/settings/account\n",
    "   - Scroll to \"API\" section  \n",
    "   - Click \"Create New Token\"\n",
    "   - **Copy your username** (displayed on the account page)\n",
    "   - **Copy the API token** immediately\n",
    "\n",
    "**Next:** Choose ONE configuration method below (2A.2 or 2A.3)\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21619153",
   "metadata": {},
   "source": [
    "#### 2A.2 Project .env Configuration (Method Option 1 of 2)\n",
    "\n",
    "<details>\n",
    "<summary><strong>Recommended</strong>: Secure, project-specific credential storage (click to expand)</summary>\n",
    "\n",
    "```bash\n",
    "# Navigate to your project root directory\n",
    "cd ~/path/to/tourism_data_project\n",
    "\n",
    "# Create .env file with your credentials (replace with values from Kaggle interface)\n",
    "echo \"KAGGLE_USERNAME=[your_username]\" >> .env\n",
    "echo \"KAGGLE_KEY=[your_copied_token]\" >> .env\n",
    "echo \"KAGGLE_API_TOKEN=[your_copied_token]\" >> .env\n",
    "\n",
    "# Secure the file\n",
    "chmod 600 .env\n",
    "echo \".env\" >> .gitignore\n",
    "```\n",
    "\n",
    "***<u>Note</u>:*** Kaggle documentation refers to use of both `KAGGLE_KEY` and `KAGGLE_API_TOKEN` - setting both ensures compatibility.\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ec7e05",
   "metadata": {},
   "source": [
    "#### 2A.2 Terminal Environment Variables  (Method Option 2 of 2)\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary><strong>Alternative</strong>: Temporary Configuration <i>requiring notebook restart</i> (click to expand)</summary>\n",
    "\n",
    "```bash\n",
    "# Run in terminal BEFORE starting Jupyter (temporary)\n",
    "export KAGGLE_USERNAME=\"[your_username]\"\n",
    "export KAGGLE_KEY=\"[your_copied_token]\"\n",
    "export KAGGLE_API_TOKEN=\"[your_copied_token]\"\n",
    "\n",
    "# Then start Jupyter from same terminal\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "***<u>Note</u>:*** Kaggle documentation refers to use of both `KAGGLE_KEY` and `KAGGLE_API_TOKEN` - setting both ensures compatibility.\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d23666",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### 2B. API Authentication Test\n",
    "*Verify Kaggle API credentials before large download*\n",
    "\n",
    "**Purpose:** Test connection before proceeding to download 8GB files\n",
    "\n",
    "**Next:** Automated download if successful, *manual instructions if test fails*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9f5a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Yelp-specific functions\n",
    "from yelp_utils import test_kaggle_authentication, download_yelp_with_complete_handling\n",
    "\n",
    "# Test Kaggle authentication\n",
    "print(\"Testing Kaggle API authentication...\")\n",
    "\n",
    "auth_success, auth_message = test_kaggle_authentication()\n",
    "\n",
    "if auth_success:\n",
    "    print(\"\\n✓ Authentication successful ready for download\")\n",
    "    print(\"Proceeding to Section 2C...\")\n",
    "else:\n",
    "    print(\"\\n✗ Authentication failed\")\n",
    "    print(\"See manual download instructions in Section 2C below\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12921481",
   "metadata": {},
   "source": [
    "### 2C. Dataset Download\n",
    "*Download Yelp Academic Dataset (~8GB) using Kaggle API*\n",
    "\n",
    "**Files:** Business (~115 MB), Reviews (~5.3 GB), Users (~3.1 GB)\n",
    "\n",
    "**Process:** ZIP download → automatic extraction → JSON files\n",
    "\n",
    "---\n",
    "<details>\n",
    "<summary><strong>Manual Download Instructions</strong> (click to expand)</summary>\n",
    "\n",
    "***If automated download fails:***\n",
    "\n",
    "**Option A: Kaggle Website Download**\n",
    "1. Visit Kaggle: https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset\n",
    "   \n",
    "2. Click \"Download\" button (requires Kaggle account)\n",
    "   \n",
    "3. Extract the downloaded ZIP file\n",
    "   \n",
    "4. Copy these files to: \n",
    "   \n",
    "    `data/bronze/yelp/00_original_download/...`\n",
    "\n",
    "    `yelp_academic_dataset_business.json`\n",
    "    `yelp_academic_dataset_review.json`\n",
    "    `yelp_academic_dataset_user.json`\n",
    "\n",
    "**Option B: Official Yelp Dataset**\n",
    "- Visting Yelp's official hosting page: https://business.yelp.com/data/resources/open-dataset/\n",
    "\n",
    "- May require registration for academic/research use\n",
    "  \n",
    "- Same procedural handling as Option 1 immediately above.\n",
    "\n",
    "**File Size Expectations:**\n",
    "- Business: ~115 MB (150K+ businesses)\n",
    "- Reviews: ~5.3 GB (7M+ reviews)\n",
    "- Users: ~3.1 GB (2M+ users)\n",
    "- Total: ~8.5 GB uncompressed\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adeb7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Large download: ~8GB total, may take 10-20 minutes\")\n",
    "print(\"Files: Business (~115 MB), Reviews (~5.3 GB), Users (~3.1 GB)\")\n",
    "\n",
    "# Download & validate in one step\n",
    "success, message, status, validation = download_yelp_with_complete_handling(\n",
    "    download_dir=original_dir,\n",
    "    target_files=TARGET_FILES\n",
    ")\n",
    "\n",
    "print(f\"\\n{message}\")\n",
    "\n",
    "if not success:\n",
    "    print(\"\\nIf download failed, see manual instructions in markdown above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88ea762",
   "metadata": {},
   "source": [
    "## 3. Bronze Layer: Raw Data Processing\n",
    "*Convert large JSON files to chunked parquet preserving original structure*\n",
    "\n",
    "**Input:** 3 JSON files (~8.4 GB total)\n",
    "\n",
    "**Output:** Chunked parquet files in `data/bronze/yelp/01_raw_conversion/`\n",
    "\n",
    "**Processing:** Memory-efficient chunked conversion (10,000 records per chunk)\n",
    "\n",
    "**Purpose:** Preserve complete dataset structure while converting to analysis-friendly format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb9c987",
   "metadata": {},
   "source": [
    "### 3A. Configuration & Pre-Conversion Check\n",
    "*Set chunking parameters and confirm files ready for processing*\n",
    "\n",
    "**Input:** Validated JSON files from Section 2 (8.4 GB total)\n",
    "\n",
    "**Configuration:** 10,000 records per parquet chunk for memory efficiency\n",
    "\n",
    "**Purpose:** Final check before beginning memory-intensive conversion process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c331ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CHUNK_SIZE = 10000\n",
    "\n",
    "# Confirm files from Section 2 validation\n",
    "file_paths = {\n",
    "    'business': original_dir / 'yelp_academic_dataset_business.json',\n",
    "    'review': original_dir / 'yelp_academic_dataset_review.json',\n",
    "    'user': original_dir / 'yelp_academic_dataset_user.json'\n",
    "}\n",
    "\n",
    "# Quick existence check (validation already done in Section 2)\n",
    "all_present = all(path.exists() for path in file_paths.values())\n",
    "\n",
    "if not all_present:\n",
    "    print(\"✗ Files missing - run Section 2 first\")\n",
    "    missing = [name for name, path in file_paths.items() if not path.exists()]\n",
    "    print(f\"Missing: {missing}\")\n",
    "else:\n",
    "    # Show file sizes\n",
    "    total_size_mb = sum(path.stat().st_size for path in file_paths.values()) / (1024 * 1024)\n",
    "\n",
    "    print(\"Files ready for bronze layer conversion:\")\n",
    "    for name, path in file_paths.items():\n",
    "        size_mb = path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  ✓ {name}: {size_mb:.1f} MB\")\n",
    "\n",
    "    print(f\"\\nTotal dataset: {total_size_mb / 1024:.1f} GB\")\n",
    "    print(f\"Chunk size: {CHUNK_SIZE:,} records per parquet file\")\n",
    "    print(\"\\n✓ Ready for conversion (Sections 3B-3D)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1a8d1c",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### 3B. Business Data Conversion\n",
    "*Convert business JSON to parquet chunks*\n",
    "\n",
    "**Input:** `yelp_academic_dataset_business.json` (~113 MB)\n",
    "\n",
    "**Expected:** ~150K business records\n",
    "\n",
    "**Output:** `yelp_business_chunk_*.parquet` files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a89a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_io import convert_json_dataset_to_chunks\n",
    "\n",
    "business_file = original_dir / 'yelp_academic_dataset_business.json'\n",
    "\n",
    "success, chunk_count, total_records = convert_json_dataset_to_chunks(\n",
    "    json_file=business_file,\n",
    "    output_dir=conversion_dir,\n",
    "    file_prefix='yelp_business',\n",
    "    chunk_size=CHUNK_SIZE\n",
    ")\n",
    "\n",
    "if success:\n",
    "    print(f\"✓ Business conversion complete: {total_records:,} records, {chunk_count} chunks\")\n",
    "else:\n",
    "    print(f\"✗ Business conversion failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cefc481",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### 3C. Review Data Conversion\n",
    "*Convert large review JSON to parquet chunks*\n",
    "\n",
    "**Input:** `yelp_academic_dataset_review.json` (~5.1 GB)\n",
    "\n",
    "**Expected:** ~7M review records\n",
    "\n",
    "**Output:** `yelp_reviews_chunk_*.parquet` files\n",
    "\n",
    "**Note:** *Largest file - may take 5-10 minutes*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31fce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_file = original_dir / 'yelp_academic_dataset_review.json'\n",
    "\n",
    "print(\"Converting review data (this may take 5-10 minutes)...\")\n",
    "success, chunk_count, total_records = convert_json_dataset_to_chunks(\n",
    "    json_file=review_file,\n",
    "    output_dir=conversion_dir,\n",
    "    file_prefix='yelp_reviews',\n",
    "    chunk_size=CHUNK_SIZE\n",
    ")\n",
    "\n",
    "if success:\n",
    "    print(f\"✓ Review conversion complete: {total_records:,} records, {chunk_count} chunks\")\n",
    "else:\n",
    "    print(f\"✗ Review conversion failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2370e8bf",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### 3D. User Data Conversion\n",
    "*Convert user JSON to parquet chunks*\n",
    "\n",
    "**Input:** `yelp_academic_dataset_user.json` (~3.2 GB)\n",
    "\n",
    "**Expected:** ~2M user records\n",
    "\n",
    "**Output:** `yelp_users_chunk_*.parquet` files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19896dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_file = original_dir / 'yelp_academic_dataset_user.json'\n",
    "\n",
    "success, chunk_count, total_records = convert_json_dataset_to_chunks(\n",
    "    json_file=user_file,\n",
    "    output_dir=conversion_dir,\n",
    "    file_prefix='yelp_users',\n",
    "    chunk_size=CHUNK_SIZE\n",
    ")\n",
    "\n",
    "if success:\n",
    "    print(f\"✓ User conversion complete: {total_records:,} records, {chunk_count} chunks\")\n",
    "else:\n",
    "    print(f\"✗ User conversion failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f292c8",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### 4A. Verify Bronze Conversion\n",
    "*Validate all three datasets converted successfully before proceeding*\n",
    "\n",
    "**Input:** Parquet chunks from Section 3 conversion\n",
    "\n",
    "**Purpose:** Confirm all business, review, and user chunks created before exploration\n",
    "\n",
    "**Uses:** `check_existing_chunks()` from data_io.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe83eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_io import check_existing_chunks\n",
    "\n",
    "# Verify bronze conversion completed successfully\n",
    "print(\"Bronze Layer Conversion Verification\")\n",
    "\n",
    "# Check business chunks\n",
    "exists_business, count_business = check_existing_chunks(\n",
    "    conversion_dir,\n",
    "    pattern=\"yelp_business_chunk_*.parquet\",\n",
    "    show_info=False\n",
    ")\n",
    "\n",
    "# Check review chunks\n",
    "exists_reviews, count_reviews = check_existing_chunks(\n",
    "    conversion_dir,\n",
    "    pattern=\"yelp_reviews_chunk_*.parquet\",\n",
    "    show_info=False\n",
    ")\n",
    "\n",
    "# Check user chunks\n",
    "exists_users, count_users = check_existing_chunks(\n",
    "    conversion_dir,\n",
    "    pattern=\"yelp_users_chunk_*.parquet\",\n",
    "    show_info=False\n",
    ")\n",
    "\n",
    "print(f\"\\nBusiness chunks: {count_business} files\")\n",
    "print(f\"Review chunks: {count_reviews} files\")\n",
    "print(f\"User chunks: {count_users} files\")\n",
    "\n",
    "# Validate expected counts\n",
    "expected = {'business': 16, 'reviews': 700, 'users': 199}\n",
    "actual = {'business': count_business, 'reviews': count_reviews, 'users': count_users}\n",
    "\n",
    "all_valid = all(actual[key] == expected[key] for key in expected)\n",
    "\n",
    "if all_valid:\n",
    "    print(f\"\\n✓ All conversions validated, ready for exploration\")\n",
    "else:\n",
    "    print(f\"\\n✗ Conversion mismatch - check Section 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee10b154",
   "metadata": {},
   "source": [
    "### 4B. Load Business Data & Analyze Geographic Distribution\n",
    "*Load all business chunks to understand state/city coverage*\n",
    "\n",
    "**Input:** `data/bronze/yelp/01_raw_conversion/yelp_business_chunk_*.parquet` (16 chunks, ~150K businesses)\n",
    "\n",
    "**Purpose:** Identify which states/cities have sufficient tourism business presence for analysis\n",
    "\n",
    "**Output:** Geographic distribution to inform city selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a831ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all business chunks\n",
    "business_chunks = []\n",
    "business_files = sorted(conversion_dir.glob(\"yelp_business_chunk_*.parquet\"))\n",
    "\n",
    "for file in business_files:\n",
    "    df = pd.read_parquet(file)\n",
    "    business_chunks.append(df)\n",
    "\n",
    "business_df = pd.concat(business_chunks, ignore_index=True)\n",
    "\n",
    "print(f\"✓ Loaded {len(business_df):,} businesses\")\n",
    "print(f\"Columns: {len(business_df.columns)}\")\n",
    "print(f\"\\nKey columns: {list(business_df.columns[:10])}\")\n",
    "\n",
    "# Quick structure check\n",
    "print(f\"\\nDataset shape: {business_df.shape}\")\n",
    "print(f\"States represented: {business_df['state'].nunique()}\")\n",
    "print(f\"Cities represented: {business_df['city'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273aa11e",
   "metadata": {},
   "source": [
    "### 4C. Analyze State & City Review Distribution\n",
    "*Use business metadata to identify states/cities with highest review activity*\n",
    "\n",
    "**Strategy:** Aggregate review counts from business metadata → identify top states → show leading cities\n",
    "\n",
    "**Purpose:** Identify optimal geographic target without loading full review dataset\n",
    "\n",
    "**Output:** Top 5 states with their leading cities by review volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b95ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate review counts by city\n",
    "city_reviews = business_df.groupby(['state', 'city'])['review_count'].sum().reset_index()\n",
    "city_reviews = city_reviews.sort_values('review_count', ascending=False)\n",
    "\n",
    "# Aggregate by state with top 2 cities\n",
    "print(\"Top 5 States by Review Volume\")\n",
    "\n",
    "state_totals = city_reviews.groupby('state')['review_count'].sum().sort_values(ascending=False)\n",
    "\n",
    "for state in state_totals.head(5).index:\n",
    "    state_total = state_totals[state]\n",
    "    state_cities = city_reviews[city_reviews['state'] == state].head(2)\n",
    "\n",
    "    print(f\"\\n  {state}: {state_total:,} total reviews\")\n",
    "    for _, row in state_cities.iterrows():\n",
    "        print(f\"  • {row['city']}: {row['review_count']:,} reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93db6caf",
   "metadata": {},
   "source": [
    "### 4D. Load Reviews & Filter Louisiana 2012-2019\n",
    "*Load all review chunks, filter to Louisiana 2012-2019, prefix columns with data source*\n",
    "\n",
    "**Input:** All review chunks (~7M reviews)\n",
    "\n",
    "**Filters:** Louisiana business reviews between 2012 and 2019\n",
    "\n",
    "**Column Strategy:** Prefix with `rev_` to identify review-source data\n",
    "\n",
    "**Output:** Louisiana reviews ready for tourism classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d4916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all review chunks\n",
    "print(\"Loading all review chunks (this may take 2-3 minutes)\")\n",
    "review_chunks = []\n",
    "review_files = sorted(conversion_dir.glob(\"yelp_reviews_chunk_*.parquet\"))\n",
    "\n",
    "for i, file in enumerate(review_files):\n",
    "    df = pd.read_parquet(file)\n",
    "    review_chunks.append(df)\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"  Loaded {i + 1}/{len(review_files)} chunks...\")\n",
    "\n",
    "reviews_df = pd.concat(review_chunks, ignore_index=True)\n",
    "print(f\"\\n✓ Loaded {len(reviews_df):,} total reviews\")\n",
    "\n",
    "# Get Louisiana business IDs\n",
    "la_business_ids = set(business_df[business_df['state'] == 'LA']['business_id'])\n",
    "print(f\"Louisiana businesses: {len(la_business_ids):,}\")\n",
    "\n",
    "# Create datetime column for filtering (preserve original date string)\n",
    "reviews_df['date_dt'] = pd.to_datetime(reviews_df['date'], errors='coerce')\n",
    "\n",
    "# Filter to Louisiana + 2012-2019 with only needed columns\n",
    "reviews_la_2012_2019 = reviews_df[\n",
    "    (reviews_df['business_id'].isin(la_business_ids)) &\n",
    "    (reviews_df['date_dt'].dt.year >= 2012) &\n",
    "    (reviews_df['date_dt'].dt.year <= 2019)\n",
    "][['review_id', 'user_id', 'business_id', 'stars', 'text', 'date', 'date_dt', 'useful']].copy()\n",
    "\n",
    "# Prefix review columns (keep both date and date_dt)\n",
    "reviews_la_2012_2019 = reviews_la_2012_2019.rename(columns={\n",
    "    'review_id': 'rev_id',\n",
    "    'user_id': 'rev_user_id',\n",
    "    'business_id': 'rev_business_id',\n",
    "    'stars': 'rev_stars',\n",
    "    'text': 'rev_text',\n",
    "    'date': 'rev_date',           # Original string preserved\n",
    "    'date_dt': 'rev_date_dt',     # Datetime for analysis\n",
    "    'useful': 'rev_useful',\n",
    "})\n",
    "\n",
    "print(f\"\\nLouisiana 2012-2019 reviews: {len(reviews_la_2012_2019):,} ({len(reviews_la_2012_2019)/len(reviews_df)*100:.1f}%)\")\n",
    "print(f\"Final columns: {list(reviews_la_2012_2019.columns)}\")\n",
    "print(f\"\\nData integrity check:\")\n",
    "print(f\"  Original date format: {reviews_la_2012_2019['rev_date'].iloc[0]} (string)\")\n",
    "print(f\"  Datetime column: {reviews_la_2012_2019['rev_date_dt'].iloc[0]} (datetime64)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474dd7c0",
   "metadata": {},
   "source": [
    "### 4E. Apply Tourism Classification to Louisiana Businesses\n",
    "*Classify businesses using comprehensive category groupings*\n",
    "\n",
    "**Purpose:** Identify tourism-relevant businesses for final dataset filtering\n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "<details>\n",
    "<summary><strong>Normalizing category matches with fuzzy logic</strong> (click to expand)</summary>\n",
    "\n",
    "- Input categories and dictionary keywords both normalized (lowercase, trailing 's' removed)\n",
    "  \n",
    "- Example: \"Walking Tours\" → \"walking tour\", matches keyword \"walking tour\"\n",
    "  \n",
    "- Example: \"Restaurants\" → \"restaurant\", matches keyword \"restaurant\"\n",
    "</details>\n",
    "\n",
    "---\n",
    "<br>\n",
    "<details>\n",
    "<summary><strong>Addressing category specificity</strong> (click to expand)</summary>\n",
    "\n",
    "Keywords like \"tour\", \"walking tour\", \"boat tour\" are not redundant:\n",
    "\n",
    "- \"tour\" matches: \"Tour\", \"Tours\" (generic tour businesses)\n",
    "  \n",
    "- \"walking tour\" matches: \"Walking Tour\", \"Walking Tours\" (specific type)\n",
    "  \n",
    "- All options required to ensure full range of business types\n",
    "</details>\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad4af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yelp_utils import classify_tourism_business\n",
    "from collections import Counter\n",
    "\n",
    "# Define comprehensive tourism category groups\n",
    "category_groups = {\n",
    "    \"restaurant\": [\n",
    "        \"restaurant\", \"food\", \"coffee & tea\", \"dessert\", \"bakery\",\n",
    "        \"ice cream & frozen yogurt\", \"juice bar & smoothie\", \"gelato\",\n",
    "        \"cupcake\", \"shaved ice\", \"food truck\", \"cafe\"\n",
    "    ],\n",
    "\n",
    "    \"hotels_travel\": [\n",
    "        \"hotel & travel\", \"hotels & travel\", \"hotel\", \"resort\",\n",
    "        \"vacation rental\", \"bed & breakfast\",\n",
    "        \"taxi\", \"limo\", \"airport shuttle\", \"car rental\", \"airline\",\n",
    "        \"transportation\", \"public transportation\", \"pedicab\"\n",
    "    ],\n",
    "\n",
    "    \"tourism\": [\n",
    "        \"tour\", \"walking tour\", \"historical tour\", \"boat tour\",\n",
    "        \"wine tour\", \"beer tour\", \"bus tour\",\n",
    "        \"museum\", \"art museum\",\n",
    "        \"landmark & historical building\", \"landmarks & historical building\",\n",
    "        \"amusement park\", \"zoo\", \"botanical garden\", \"park\",\n",
    "        \"stadium & arena\", \"stadiums & arena\",\n",
    "        \"bike rental\", \"local flavor\"\n",
    "    ],\n",
    "\n",
    "    \"entertainment\": [\n",
    "        \"art & entertainment\", \"arts & entertainment\",\n",
    "        \"music venue\", \"performing art\",\n",
    "        \"comedy club\", \"jazz & blue\", \"festival\",\n",
    "        \"cinema\", \"professional sports team\"\n",
    "    ],\n",
    "\n",
    "    \"nightlife\": [\n",
    "        \"nightlife\", \"bar\", \"pub\", \"wine bar\", \"beer bar\",\n",
    "        \"beer garden\", \"cocktail bar\", \"dive bar\", \"brewpub\",\n",
    "        \"distillery\", \"winery\"\n",
    "    ],\n",
    "\n",
    "    \"events\": [\n",
    "        \"event planning & service\", \"venue & event space\",\n",
    "        \"wedding planning\", \"party & event planning\", \"bridal\",\n",
    "        \"party bus rental\", \"custom cake\", \"dance club\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Apply tourism classification to Louisiana businesses\n",
    "la_businesses = business_df[business_df['state'] == 'LA'].copy()\n",
    "la_businesses['tourism_groups'] = la_businesses['categories'].apply(\n",
    "    lambda x: classify_tourism_business(x, category_groups)\n",
    ")\n",
    "\n",
    "# Count businesses by tourism group\n",
    "classified = la_businesses[la_businesses['tourism_groups'].apply(len) > 0]\n",
    "print(f\"Classified as tourism: {len(classified):,} / {len(la_businesses):,} ({len(classified)/len(la_businesses)*100:.1f}%)\")\n",
    "\n",
    "all_groups = [group for groups in classified['tourism_groups'] for group in groups]\n",
    "group_counts = Counter(all_groups)\n",
    "for group, count in group_counts.most_common():\n",
    "    print(f\"  {group}: {count:,} businesses\")\n",
    "\n",
    "unclassified = la_businesses[la_businesses['tourism_groups'].apply(len) == 0]\n",
    "print(f\"\\nUnclassified: {len(unclassified):,} businesses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752e2fd6",
   "metadata": {},
   "source": [
    "### 4F. Filter Reviews to Tourism Businesses Only\n",
    "*Reduce review dataset to only tourism-classified businesses*\n",
    "\n",
    "**Input:** Louisiana reviews (2012-2019) with `rev_` prefixed columns\n",
    "\n",
    "**Purpose:** Final dataset focuses exclusively on tourism sentiment\n",
    "\n",
    "---\n",
    "<br>\n",
    "<details>\n",
    "<summary><strong>Filter and combine handling</strong> (click to expand)</summary>\n",
    "\n",
    "**Filter:** Keep only reviews for tourism businesses (classified in 4E)\n",
    "\n",
    "**Join Strategy:** Match `rev_business_id` to tourism business IDs\n",
    "\n",
    "**Data Integrity:** No column modifications - pure filtering operation\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea68400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tourism business IDs\n",
    "tourism_business_ids = set(classified['business_id'])\n",
    "\n",
    "print(f\"Tourism businesses: {len(tourism_business_ids):,}\")\n",
    "\n",
    "# Filter reviews to tourism businesses only\n",
    "reviews_tourism = reviews_la_2012_2019[\n",
    "    reviews_la_2012_2019['rev_business_id'].isin(tourism_business_ids)\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nReview Filtering\")\n",
    "print(f\"  Before: {len(reviews_la_2012_2019):,} reviews\")\n",
    "print(f\"  After: {len(reviews_tourism):,} reviews\")\n",
    "print(f\"  Reduction: {(1 - len(reviews_tourism)/len(reviews_la_2012_2019))*100:.1f}%\")\n",
    "\n",
    "# Analyze city distribution in filtered reviews\n",
    "tourism_business_with_city = classified[['business_id', 'city']]\n",
    "reviews_with_city = reviews_tourism.merge(\n",
    "    tourism_business_with_city,\n",
    "    left_on='rev_business_id',\n",
    "    right_on='business_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"\\nCity distribution (tourism reviews only)\")\n",
    "city_dist = reviews_with_city['city'].value_counts().head(5)\n",
    "for city, count in city_dist.items():\n",
    "    pct = (count / len(reviews_with_city)) * 100\n",
    "    print(f\"  {city}: {count:,} reviews ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a3a8cd",
   "metadata": {},
   "source": [
    "### 4G. Load User Data & Filter to Tourism Reviewers\n",
    "*Load user chunks and filter to users who reviewed tourism businesses*\n",
    "\n",
    "**Input:** User chunks (199 chunks, ~2M users)\n",
    "\n",
    "**Filter:** Keep only users present in tourism review dataset (identified by `rev_user_id`)\n",
    "\n",
    "**Purpose:** Reduce user dataset before final merge, minimize memory footprint\n",
    "\n",
    "---\n",
    "<br>\n",
    "<details>\n",
    "<summary><strong>Column handling</strong> (click to expand)</summary>\n",
    "\n",
    "**Columns Kept:** 6 essential user attributes only\n",
    "\n",
    "- `usr_id`, `usr_name`, `usr_review_count`, `usr_yelping_since`, `usr_useful`, `usr_average_stars`\n",
    "\n",
    "**Column Strategy:** Prefix with `usr_` for source identification\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3de3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique user IDs from tourism reviews\n",
    "tourism_user_ids = set(reviews_tourism['rev_user_id'])\n",
    "print(f\"Unique users in tourism reviews: {len(tourism_user_ids):,}\")\n",
    "\n",
    "# Check if user data already loaded and processed\n",
    "if 'users_tourism' in locals() and 'usr_id' in users_tourism.columns:\n",
    "    print(\"\\n✓ User data already loaded and processed\")\n",
    "    print(f\"Users: {len(users_tourism):,}\")\n",
    "    print(f\"Columns: {list(users_tourism.columns)}\")\n",
    "else:\n",
    "    # Load and filter user chunks\n",
    "    print(\"\\nLoading and filtering user data\")\n",
    "    user_chunks = []\n",
    "    user_files = sorted(conversion_dir.glob(\"yelp_users_chunk_*.parquet\"))\n",
    "\n",
    "    for i, file in enumerate(user_files):\n",
    "        df = pd.read_parquet(file)\n",
    "        # Filter to tourism users only\n",
    "        df_filtered = df[df['user_id'].isin(tourism_user_ids)]\n",
    "        if len(df_filtered) > 0:\n",
    "            user_chunks.append(df_filtered)\n",
    "\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"  Processed {i + 1}/{len(user_files)} chunks...\")\n",
    "\n",
    "    users_tourism = pd.concat(user_chunks, ignore_index=True)\n",
    "\n",
    "    # Keep only needed user columns\n",
    "    users_tourism = users_tourism[[\n",
    "        'user_id', 'review_count', 'yelping_since', 'useful', 'average_stars'\n",
    "    ]].copy()\n",
    "\n",
    "    # Rename with usr_ prefix\n",
    "    users_tourism = users_tourism.rename(columns={\n",
    "        'user_id': 'usr_id',\n",
    "        'review_count': 'usr_review_count',\n",
    "        'yelping_since': 'usr_yelping_since',\n",
    "        'useful': 'usr_useful',\n",
    "        'average_stars': 'usr_average_stars'\n",
    "    })\n",
    "\n",
    "    print(f\"\\n✓ Loaded {len(users_tourism):,} tourism users\")\n",
    "    print(f\"  Columns: {list(users_tourism.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0274a173",
   "metadata": {},
   "source": [
    "### 4H. Merge Reviews, Business, and User Data\n",
    "*Combine three datasets with source prefixes in staged approach*\n",
    "\n",
    "**Output:** Complete tourism dataset with clear data provenance (`rev_`, `bus_`, `usr_` prefixes)\n",
    "\n",
    "---\n",
    "<br>\n",
    "<details>\n",
    "<summary><strong>Merge handling with data integrity</strong> (click to expand)</summary>\n",
    "\n",
    "**Step 1:** Prepare business data with `bus_` prefix (remove `state`, `hours`, `is_open`)\n",
    "\n",
    "**Step 2:** Merge reviews + business data\n",
    "\n",
    "**Step 3:** Merge result + user data\n",
    "\n",
    "**Data Integrity:** All merges use left join to preserve all review records\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e1c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing datasets for merge...\")\n",
    "print()\n",
    "\n",
    "# Check if merge already completed\n",
    "if 'merged' in locals() and 'bus_name' in merged.columns and 'usr_review_count' in merged.columns:\n",
    "    print(\"✓ Merge already completed\")\n",
    "    print(f\"Rows: {len(merged):,}\")\n",
    "    print(f\"Columns: {len(merged.columns)}\")\n",
    "    print()\n",
    "\n",
    "    print(\"Column structure\")\n",
    "    rev_cols = [c for c in merged.columns if c.startswith('rev_')]\n",
    "    bus_cols = [c for c in merged.columns if c.startswith('bus_')]\n",
    "    usr_cols = [c for c in merged.columns if c.startswith('usr_')]\n",
    "    print(f\"  rev_ columns: {len(rev_cols)}\")\n",
    "    print(f\"  bus_ columns: {len(bus_cols)}\")\n",
    "    print(f\"  usr_ columns: {len(usr_cols)}\")\n",
    "else:\n",
    "    # Prepare business data with renamed columns\n",
    "    business_tourism = classified[[\n",
    "        'business_id', 'name', 'city', 'latitude', 'longitude',\n",
    "        'stars', 'review_count', 'categories', 'postal_code'\n",
    "    ]].copy()\n",
    "\n",
    "    business_tourism = business_tourism.rename(columns={\n",
    "        'business_id': 'bus_id',\n",
    "        'name': 'bus_name',\n",
    "        'city': 'bus_city',\n",
    "        'latitude': 'bus_latitude',\n",
    "        'longitude': 'bus_longitude',\n",
    "        'stars': 'bus_average_stars',\n",
    "        'review_count': 'bus_review_count',\n",
    "        'categories': 'bus_categories',\n",
    "        'postal_code': 'bus_postal_code'\n",
    "    })\n",
    "\n",
    "    print(\"Merging datasets...\")\n",
    "    print()\n",
    "\n",
    "    # Merge reviews + business\n",
    "    merged = reviews_tourism.merge(\n",
    "        business_tourism,\n",
    "        left_on='rev_business_id',\n",
    "        right_on='bus_id',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Merge + users (already has usr_ prefix from 4G)\n",
    "    merged = merged.merge(\n",
    "        users_tourism,\n",
    "        left_on='rev_user_id',\n",
    "        right_on='usr_id',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    print(\"Merge complete\")\n",
    "    print(f\"  Final rows: {len(merged):,}\")\n",
    "    print(f\"  Final columns: {len(merged.columns)}\")\n",
    "    print()\n",
    "\n",
    "    # Verify column structure\n",
    "    rev_cols = [c for c in merged.columns if c.startswith('rev_')]\n",
    "    bus_cols = [c for c in merged.columns if c.startswith('bus_')]\n",
    "    usr_cols = [c for c in merged.columns if c.startswith('usr_')]\n",
    "\n",
    "    print(\"Column structure\")\n",
    "    print(f\"  rev_ columns: {len(rev_cols)}\")\n",
    "    print(f\"  bus_ columns: {len(bus_cols)}\")\n",
    "    print(f\"  usr_ columns: {len(usr_cols)}\")\n",
    "    print()\n",
    "\n",
    "    # Verify no data loss\n",
    "    business_match = merged['bus_name'].notna().sum()\n",
    "    user_match = merged['usr_review_count'].notna().sum()\n",
    "\n",
    "    print(\"Data integrity check\")\n",
    "    print(f\"  Reviews matched to business: {business_match:,} ({business_match/len(merged)*100:.1f}%)\")\n",
    "    print(f\"  Reviews matched to user: {user_match:,} ({user_match/len(merged)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8ebfe",
   "metadata": {},
   "source": [
    "## 5. Primary Filter: Save Louisiana Tourism Dataset\n",
    "*Save merged dataset to bronze primary filter*\n",
    "\n",
    "**Input:** Tourism reviews with business and user data\n",
    "\n",
    "**Output:** `data/bronze/yelp/02_primary_filter/louisiana_tourism_2012_2019.parquet`\n",
    "\n",
    "\n",
    "---\n",
    "<details>\n",
    "<summary><strong>Dataset Summary</strong> (click to expand)</summary>\n",
    "\n",
    "- Louisiana tourism businesses only\n",
    "  \n",
    "- Years: 2012-2019\n",
    "\n",
    "- Complete merge: Reviews + Business + User data\n",
    "  \n",
    "- Original data integrity preserved (no type conversion overwrites)\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585d416e",
   "metadata": {},
   "source": [
    "### 5A. Calculate User Review Counts by Year\n",
    "*Tally each user's Louisiana tourism reviews per year for reviewer classification*\n",
    "\n",
    "**Purpose:** Identify high-volume reviewers (*\"locals\"* vs. *\"tourists\"*) by analyzing annual Louisiana review patterns\n",
    "\n",
    "---\n",
    "<details>\n",
    "<summary><strong>Strategy and Use Case</strong> (click to expand)</summary>\n",
    "\n",
    "**Strategy:** Create pivot table of user review counts per year\n",
    "\n",
    "- Columns: `usr_2012_la_rev_count`, `usr_2013_la_rev_count`, ..., `usr_2019_la_rev_count`\n",
    "\n",
    "- Values: Number of reviews each user wrote in Louisiana tourism businesses per year\n",
    "\n",
    "**Use Case:** Enable reviewer classification during analysis\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25627aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating user review counts by year...\")\n",
    "print()\n",
    "\n",
    "# Extract year from review date for pivot\n",
    "merged['rev_year'] = merged['rev_date_dt'].dt.year\n",
    "\n",
    "# Create pivot: users × years\n",
    "print(\"Creating user-year pivot table...\")\n",
    "user_year_pivot = merged.groupby(['rev_user_id', 'rev_year']).size().reset_index(name='year_review_count')\n",
    "user_year_pivot = user_year_pivot.pivot(\n",
    "    index='rev_user_id',\n",
    "    columns='rev_year',\n",
    "    values='year_review_count'\n",
    ").fillna(0).astype(int)\n",
    "\n",
    "# Rename columns with usr_ prefix and _la_rev_count suffix\n",
    "year_columns = {year: f'usr_{int(year)}_la_rev_count' for year in user_year_pivot.columns}\n",
    "user_year_pivot = user_year_pivot.rename(columns=year_columns)\n",
    "user_year_pivot = user_year_pivot.reset_index()\n",
    "\n",
    "print()\n",
    "print(\"User-year pivot created\")\n",
    "print(f\"  Unique users: {len(user_year_pivot):,}\")\n",
    "year_cols = [col for col in user_year_pivot.columns if col.startswith('usr_')]\n",
    "print(f\"  User review count/year columns: {year_cols}\")\n",
    "print()\n",
    "\n",
    "# Merge back to main dataset\n",
    "print(\"Merging annual counts back to dataset...\")\n",
    "merged = merged.merge(user_year_pivot, on='rev_user_id', how='left')\n",
    "\n",
    "# Drop temporary rev_year column\n",
    "merged = merged.drop(columns=['rev_year'])\n",
    "\n",
    "print()\n",
    "print(\"Merged dataset updated\")\n",
    "print(f\"  Total columns: {len(merged.columns)}\")\n",
    "print(f\"  New year count columns: {len([c for c in merged.columns if '_la_rev_count' in c])}\")\n",
    "print()\n",
    "\n",
    "# Analyze high-volume reviewers across all years\n",
    "print(\"High-Volume Reviewer Analysis\")\n",
    "for year in range(2012, 2020):\n",
    "    col_name = f'usr_{year}_la_rev_count'\n",
    "    if col_name in merged.columns:\n",
    "        high_volume = (merged[col_name] > 50).sum()\n",
    "        if high_volume > 0:\n",
    "            max_count = merged[col_name].max()\n",
    "            print(f\"  {year}: {high_volume:,} users with >50 reviews (max: {max_count})\")\n",
    "print()\n",
    "\n",
    "print(\"✓ Annual review counts calculated and merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72cbae7",
   "metadata": {},
   "source": [
    "### 5B. Primary Filter: Save Louisiana Tourism Dataset\n",
    "*Save merged dataset to bronze primary filter*\n",
    "\n",
    "**Input:** 547,432 tourism reviews with complete business, user, and annual count data\n",
    "\n",
    "**Output:** `data/bronze/yelp/02_primary_filter/louisiana_tourism_2012_2019.parquet`\n",
    "\n",
    "\n",
    "---\n",
    "<details>\n",
    "<summary><strong>Dataset Summary</strong> (click to expand)</summary>\n",
    "\n",
    "- Louisiana tourism businesses only (6,307 businesses)\n",
    "\n",
    "- Years: 2012-2019\n",
    "\n",
    "- Complete merge: Reviews (`rev_`) + Business (`bus_`) + User (`usr_`) + Annual counts\n",
    "\n",
    "- Original data integrity preserved (source date strings maintained, datetime in separate column)\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary><strong>Column Structure</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "- `rev_id`, `rev_user_id`, `rev_business_id`, `rev_stars`, `rev_text`, `rev_date`, `rev_date_dt`, `rev_useful`\n",
    "\n",
    "- `bus_id`, `bus_name`, `bus_city`, `bus_latitude`, `bus_longitude`, `bus_average_stars`, `bus_review_count`, `bus_categories`, `bus_postal_code`\n",
    "\n",
    "- `usr_id`, `usr_review_count`, `usr_yelping_since`, `usr_useful`, `usr_average_stars`\n",
    "\n",
    "- `usr_2012_la_rev_count` through `usr_2019_la_rev_count`\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478aa2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_io import check_existing_file\n",
    "\n",
    "# Set up primary filter output\n",
    "output_file = primary_filter_dir / \"louisiana_tourism_2012_2019.parquet\"\n",
    "\n",
    "# Check if already saved\n",
    "exists, info = check_existing_file(output_file, file_type='parquet', show_info=False)\n",
    "\n",
    "if not exists:\n",
    "    print(\"Saving Louisiana tourism dataset...\")\n",
    "\n",
    "    merged.to_parquet(output_file, compression=\"snappy\", index=False)\n",
    "\n",
    "    file_size_mb = output_file.stat().st_size / (1024 * 1024)\n",
    "\n",
    "    print()\n",
    "    print(\"✓ Dataset saved successfully\")\n",
    "    print(f\"Location: {output_file.name}\")\n",
    "    print(f\"Directory: {output_file.parent}\")\n",
    "    print(f\"Size: {file_size_mb:.1f} MB\")\n",
    "    print(f\"Rows: {len(merged):,}\")\n",
    "    print(f\"Columns: {len(merged.columns)}\")\n",
    "    print()\n",
    "\n",
    "    # Column breakdown\n",
    "    rev_cols = [c for c in merged.columns if c.startswith('rev_')]\n",
    "    bus_cols = [c for c in merged.columns if c.startswith('bus_')]\n",
    "    usr_cols = [c for c in merged.columns if c.startswith('usr_') and '_la_rev_count' not in c]\n",
    "    year_cols = [c for c in merged.columns if '_la_rev_count' in c]\n",
    "\n",
    "    print(\"Column summary\")\n",
    "    print(f\"  rev_ columns: {len(rev_cols)}\")\n",
    "    print(f\"  bus_ columns: {len(bus_cols)}\")\n",
    "    print(f\"  usr_ columns: {len(usr_cols)}\")\n",
    "    print(f\"  year count columns: {len(year_cols)}\")\n",
    "\n",
    "else:\n",
    "    print(f\"[SKIP] Primary filter already exists: {output_file.name}\")\n",
    "    print(f\"Size: {info['size_mb']:.1f} MB\")\n",
    "\n",
    "print()\n",
    "print(\"Primary filter complete, ready for year selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24a4e7e",
   "metadata": {},
   "source": [
    "## **6. Load Primary Filter & Year Selection**\n",
    "*Load Louisiana tourism dataset and configure target year for analysis*\n",
    "\n",
    "**Input:** `data/bronze/yelp/02_primary_filter/louisiana_tourism_2012_2019.parquet` (217.9 MB)\n",
    "\n",
    "**Strategy:** Load → Verify → Select year → Validate → Ready for silver save\n",
    "\n",
    "**Target Years:** 2013 (Super Bowl), 2016 (Baseline), 2018 (Tricentennial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6c7d7e",
   "metadata": {},
   "source": [
    "### 6A. Memory Cleanup\n",
    "*Clear intermediate DataFrames from memory before loading primary filter*\n",
    "\n",
    "**Purpose:** Free memory and ensure clean slate for year selection workflow\n",
    "\n",
    "**Action:** Remove large DataFrames from Sections 4-5, force garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0390e213",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaning up intermediate data from memory...\")\n",
    "\n",
    "# List of variables to clean up\n",
    "cleanup_vars = [\n",
    "    'merged',\n",
    "    'reviews_tourism',\n",
    "    'reviews_la_2012_2019',\n",
    "    'business_df',\n",
    "    'la_businesses',\n",
    "    'classified',\n",
    "    'users_tourism',\n",
    "    'business_tourism',\n",
    "    'user_year_pivot',\n",
    "    'user_year_counts'\n",
    "]\n",
    "\n",
    "# Iterate and delete if exists\n",
    "cleanup_count = 0\n",
    "for var in cleanup_vars:\n",
    "    if var in dir():\n",
    "        del globals()[var]\n",
    "        cleanup_count += 1\n",
    "\n",
    "# Force garbage collection\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print()\n",
    "print(f\"✓ Cleaned {cleanup_count} variables from memory\")\n",
    "print(\"Ready to load primary filter fresh from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655d41c9",
   "metadata": {},
   "source": [
    "### 6B. Load Primary Filter & Verify Dataset\n",
    "*Load saved Louisiana tourism dataset and confirm structure using shared utilities*\n",
    "\n",
    "**Input:** `data/bronze/yelp/02_primary_filter/louisiana_tourism_2012_2019.parquet`\n",
    "\n",
    "**Uses:** `check_existing_file()` from data_io.py for consistent file handling\n",
    "\n",
    "**Purpose:** Verify primary filtering results and prepare for year selection\n",
    "\n",
    "\n",
    "---\n",
    "<details>\n",
    "<summary><strong>Expected Structure</strong> (click to expand)</summary>\n",
    "\n",
    "- 547,432 tourism reviews\n",
    "\n",
    "- 30 total columns (8 rev_, 9 bus_, 5 usr_, 8 year counts)\n",
    "\n",
    "- Years: 2012-2019 coverage\n",
    "</details>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c1dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_io import check_existing_file\n",
    "\n",
    "print(\"Load Primary Filter & Verify Dataset\")\n",
    "print()\n",
    "\n",
    "# Load primary filter dataset\n",
    "primary_filter_file = primary_filter_dir / \"louisiana_tourism_2012_2019.parquet\"\n",
    "\n",
    "# Check if file exists using shared utility\n",
    "exists, info = check_existing_file(primary_filter_file, file_type='parquet', show_info=False)\n",
    "\n",
    "if not exists:\n",
    "    print(\"✗ Primary filter file not found - run Section 5 first\")\n",
    "    print(f\"Expected location: {primary_filter_file}\")\n",
    "else:\n",
    "    # Load dataset\n",
    "    exploration_df = pd.read_parquet(primary_filter_file)\n",
    "\n",
    "    print(\"✓ Dataset loaded successfully\")\n",
    "    print(f\"File: {primary_filter_file.name}\")\n",
    "    print(f\"Directory: {primary_filter_file.parent}\")\n",
    "    print(f\"Size: {info['size_mb']:.1f} MB\")\n",
    "    print()\n",
    "\n",
    "    print(\"Dataset shape\")\n",
    "    print(f\"  Rows: {len(exploration_df):,}\")\n",
    "    print(f\"  Columns: {len(exploration_df.columns)}\")\n",
    "    print()\n",
    "\n",
    "    # Verify column structure\n",
    "    rev_cols = [c for c in exploration_df.columns if c.startswith('rev_')]\n",
    "    bus_cols = [c for c in exploration_df.columns if c.startswith('bus_')]\n",
    "    usr_cols = [c for c in exploration_df.columns if c.startswith('usr_') and '_la_rev_count' not in c]\n",
    "    year_cols = [c for c in exploration_df.columns if '_la_rev_count' in c]\n",
    "\n",
    "    print(\"Column summary\")\n",
    "    print(f\"  rev_ columns: {len(rev_cols)}\")\n",
    "    print(f\"  bus_ columns: {len(bus_cols)}\")\n",
    "    print(f\"  usr_ columns: {len(usr_cols)}\")\n",
    "    print(f\"  year count columns: {len(year_cols)}\")\n",
    "    print()\n",
    "\n",
    "    # Year distribution\n",
    "    year_dist = exploration_df['rev_date_dt'].dt.year.value_counts().sort_index()\n",
    "\n",
    "    print(\"Year distribution\")\n",
    "    for year, count in year_dist.items():\n",
    "        print(f\"  {year}: {count:,} reviews\")\n",
    "    print()\n",
    "\n",
    "    print(\"✓ Primary filter verified - ready for year selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff211629",
   "metadata": {},
   "source": [
    "### 6C. Configure Target Year for Analysis\n",
    "*Select specific year for detailed tourism event analysis*\n",
    "\n",
    "**Available Years:** 2012-2019 with strong representation\n",
    "\n",
    "**Strategy:** Process one year at a time for focused analysis\n",
    "\n",
    "**Configuration:** Set `TARGET_YEAR` variable below and rerun from this cell for different year\n",
    "\n",
    "\n",
    "---\n",
    "<details>\n",
    "<summary><strong>Project Target Years</strong> (click to expand)</summary>\n",
    "\n",
    "- **2013:** Super Bowl XLVII (February) - massive tourism influx (40,084 reviews)\n",
    "\n",
    "- **2016:** Baseline year - normal seasonal patterns (77,097 reviews)\n",
    "\n",
    "- **2018:** New Orleans Tricentennial - 300th anniversary (97,887 reviews)\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9a549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER CONFIGURATION - Target Year Selection\n",
    "TARGET_YEAR = 2016          # Options: 2013, 2016, 2018 (or any 2012-2019)\n",
    "\n",
    "print(\"Target Year Configuration\")\n",
    "print()\n",
    "print(f\"Selected year: {TARGET_YEAR}\")\n",
    "print(f\"To change: Update |TARGET_YEAR| variable above and rerun from here\")\n",
    "print()\n",
    "\n",
    "# Validate target year availability\n",
    "available_years = sorted(year_dist.index.tolist())\n",
    "\n",
    "if TARGET_YEAR not in available_years:\n",
    "    print(f\"✗ Target year {TARGET_YEAR} not available\")\n",
    "    print(f\"Available years: {available_years}\")\n",
    "else:\n",
    "    # Filter to target year\n",
    "    year_mask = exploration_df['rev_date_dt'].dt.year == TARGET_YEAR\n",
    "    year_df = exploration_df[year_mask].copy()\n",
    "\n",
    "    print(f\"✓ Target year {TARGET_YEAR} is available\")\n",
    "    print(f\"Records: {len(year_df):,}\")\n",
    "    print()\n",
    "\n",
    "    # Show year context\n",
    "    print(\"Year context\")\n",
    "    for year in available_years:\n",
    "        count = year_dist[year]\n",
    "        marker = \" <- TARGET\" if year == TARGET_YEAR else \"\"\n",
    "        print(f\"  {year}: {count:,} reviews{marker}\")\n",
    "    print()\n",
    "\n",
    "    # Basic statistics for target year\n",
    "    print(\"Target year summary\")\n",
    "    print(f\"  Unique users: {year_df['rev_user_id'].nunique():,}\")\n",
    "    print(f\"  Unique businesses: {year_df['rev_business_id'].nunique():,}\")\n",
    "    print(f\"  Cities: {year_df['bus_city'].nunique()}\")\n",
    "    print(f\"  Average rating: {year_df['rev_stars'].mean():.2f}\")\n",
    "    print()\n",
    "\n",
    "    print(\"✓ Configuration complete - ready for exploratory analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030c5ae4",
   "metadata": {},
   "source": [
    "### 6D. Exploratory Validation of Target Year (Optional)\n",
    "*Analyze data richness and distribution for selected year*\n",
    "\n",
    "**Purpose:** Ensure selected year has sufficient data quality for sentiment analysis\n",
    "\n",
    "**Note:** Optional section - can skip to Section 7 for silver save\n",
    "\n",
    "---\n",
    "<details>\n",
    "<summary><strong>Annual Dataset Checks</strong> (click to expand)</summary>\n",
    "\n",
    "- Seasonal distribution (ensure all seasons represented)\n",
    "\n",
    "- City coverage (confirm New Orleans dominance)\n",
    "\n",
    "- Review completeness (text availability)\n",
    "\n",
    "- Rating distribution\n",
    "\n",
    "- High-volume reviewer identification\n",
    "</details>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbda8bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Exploratory Analysis - {TARGET_YEAR}\")\n",
    "print()\n",
    "\n",
    "# 1. Seasonal distribution\n",
    "year_df['month'] = year_df['rev_date_dt'].dt.month\n",
    "\n",
    "print(\"Monthly distribution\")\n",
    "monthly_counts = year_df['month'].value_counts().sort_index()\n",
    "for month in range(1, 13):\n",
    "    if month in monthly_counts.index:\n",
    "        count = monthly_counts[month]\n",
    "        pct = count / len(year_df) * 100\n",
    "        print(f\"  Month {month:2d}: {count:5,} reviews ({pct:4.1f}%)\")\n",
    "print()\n",
    "\n",
    "# 2. City distribution\n",
    "print(\"City distribution\")\n",
    "city_dist = year_df['bus_city'].value_counts().head(5)\n",
    "for city, count in city_dist.items():\n",
    "    pct = count / len(year_df) * 100\n",
    "    print(f\"  {city}: {count:,} reviews ({pct:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# 3. Review completeness\n",
    "text_available = year_df['rev_text'].notna().sum()\n",
    "text_pct = text_available / len(year_df) * 100\n",
    "print(\"Review completeness\")\n",
    "print(f\"  Text available: {text_available:,} ({text_pct:.1f}%)\")\n",
    "print(f\"  Text missing: {len(year_df) - text_available:,} ({100-text_pct:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# 4. Rating distribution\n",
    "print(\"Rating distribution\")\n",
    "rating_dist = year_df['rev_stars'].value_counts().sort_index()\n",
    "for stars, count in rating_dist.items():\n",
    "    pct = count / len(year_df) * 100\n",
    "    bar = \"█\" * int(pct / 2)  # Visual bar\n",
    "    print(f\"  {int(stars)} stars: {count:5,} ({pct:4.1f}%) {bar}\")\n",
    "print()\n",
    "\n",
    "# 5. High-volume reviewers in target year\n",
    "year_count_col = f'usr_{TARGET_YEAR}_la_rev_count'\n",
    "if year_count_col in year_df.columns:\n",
    "    high_volume_mask = year_df[year_count_col] > 50\n",
    "    high_volume_count = high_volume_mask.sum()\n",
    "    high_volume_users = year_df[high_volume_mask]['rev_user_id'].nunique()\n",
    "\n",
    "    print(f\"High-volume reviewers (>50 reviews in {TARGET_YEAR})\")\n",
    "    print(f\"  Unique users: {high_volume_users:,}\")\n",
    "    print(f\"  Total reviews from these users: {high_volume_count:,} ({high_volume_count/len(year_df)*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# Drop temporary month column\n",
    "year_df = year_df.drop(columns=['month'])\n",
    "\n",
    "print(\"✓ Exploratory validation complete\")\n",
    "print(\"Ready for Section 7: Silver Layer Save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e97cdb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<u>***Year Configuration Checkpoint***</u>\n",
    "\n",
    "**Project Scope:** New Orleans tourism events (2013 Super Bowl, 2016 Baseline, 2018 Tricentennial)\n",
    "\n",
    "**Reset Point:** Update `TARGET_YEAR` variable above and rerun from here for different year\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d5d1db",
   "metadata": {},
   "source": [
    "## 7. Silver Layer: Save Target Year to Staging\n",
    "*Save validated year-specific dataset to silver staging for gold layer integration*\n",
    "\n",
    "**Input:** Validated year data from Section 6\n",
    "\n",
    "**Output:** `data/silver/yelp/staging/new_orleans_{TARGET_YEAR}_final.parquet`\n",
    "\n",
    "**Purpose:** Create year-specific dataset ready for gold layer processing (where reviewer classification will occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4294c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_io import check_existing_file\n",
    "\n",
    "print(f\"Silver Layer Save - New Orleans {TARGET_YEAR}\")\n",
    "print()\n",
    "\n",
    "# Set up silver staging output file\n",
    "output_file = silver_staging / f\"new_orleans_{TARGET_YEAR}_final.parquet\"\n",
    "\n",
    "# Check if save already completed\n",
    "exists, info = check_existing_file(output_file, file_type='parquet', show_info=False)\n",
    "\n",
    "if not exists:\n",
    "    print(\"Saving to silver staging...\")\n",
    "\n",
    "    # Save year-specific data to silver\n",
    "    year_df.to_parquet(output_file, compression=\"snappy\", index=False)\n",
    "\n",
    "    file_size_mb = output_file.stat().st_size / (1024 * 1024)\n",
    "\n",
    "    print()\n",
    "    print(\"✓ Silver layer save complete\")\n",
    "    print(f\"Location: {output_file.name}\")\n",
    "    print(f\"Directory: {output_file.parent}\")\n",
    "    print(f\"Size: {file_size_mb:.1f} MB\")\n",
    "    print(f\"Rows: {len(year_df):,}\")\n",
    "    print(f\"Columns: {len(year_df.columns)}\")\n",
    "    print()\n",
    "\n",
    "    # Show key statistics\n",
    "    print(\"Dataset summary\")\n",
    "    print(f\"  City: New Orleans (+ metro area)\")\n",
    "    print(f\"  Year: {TARGET_YEAR}\")\n",
    "    print(f\"  Tourism businesses: {year_df['rev_business_id'].nunique():,}\")\n",
    "    print(f\"  Reviewers: {year_df['rev_user_id'].nunique():,}\")\n",
    "    print(f\"  Average rating: {year_df['rev_stars'].mean():.2f}\")\n",
    "\n",
    "else:\n",
    "    print(f\"[SKIP] Silver staging file already exists: {output_file.name}\")\n",
    "    print(f\"Size: {info['size_mb']:.1f} MB\")\n",
    "\n",
    "print()\n",
    "print(\"Available silver staging files\")\n",
    "all_files = sorted(silver_staging.glob(\"new_orleans_*_final.parquet\"))\n",
    "if all_files:\n",
    "    for file in all_files:\n",
    "        size_mb = file.stat().st_size / (1024 * 1024)\n",
    "        year = file.stem.split('_')[2]  # Extract year from filename\n",
    "        marker = \" <- CURRENT\" if file == output_file else \"\"\n",
    "        print(f\"  ✓ {file.name} ({size_mb:.1f} MB){marker}\")\n",
    "else:\n",
    "    print(\"  No files found\")\n",
    "\n",
    "print()\n",
    "print(\"✓ Ready for final verification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffd800a",
   "metadata": {},
   "source": [
    "## 8. Final Verification & Summary\n",
    "*Comprehensive dataset summary and workflow completion status*\n",
    "\n",
    "**Uses:** `print_final_summary()` from data_validation.py\n",
    "\n",
    "**Purpose:** Verify saved dataset integrity and display workflow status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb0c621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_validation import print_final_summary\n",
    "\n",
    "print(f\"Final Verification - New Orleans {TARGET_YEAR}\")\n",
    "print()\n",
    "\n",
    "# Print comprehensive final summary\n",
    "print_final_summary(\n",
    "    output_file,\n",
    "    dataset_name=f\"Yelp New Orleans {TARGET_YEAR}\",\n",
    "    file_type='parquet'\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "# Workflow completion status\n",
    "print(\"Workflow Status\")\n",
    "print(f\"  City: New Orleans ✓\")\n",
    "print(f\"  Year: {TARGET_YEAR} ✓\")\n",
    "print(f\"  Reviews: {len(year_df):,} ✓\")\n",
    "print(f\"  File: {output_file.name} ✓\")\n",
    "print()\n",
    "\n",
    "print(\"To process different year\")\n",
    "print(f\"  1. Update TARGET_YEAR variable in Section 6C\")\n",
    "print(f\"  2. Rerun Sections 6C-8\")\n",
    "print(f\"  3. Produces: new_orleans_[YEAR]_final.parquet\")\n",
    "print()\n",
    "\n",
    "print(\"Project scope: Process years 2013, 2016, 2018 for comparative analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d29e97",
   "metadata": {},
   "source": [
    "## 9. Workflow Completion\n",
    "\n",
    "**Current Status:**\n",
    "Bronze → Silver workflow complete for New Orleans tourism dataset\n",
    "\n",
    "**Action:** Review next steps and clean memory when ready to proceed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9932f3",
   "metadata": {},
   "source": [
    "### 9A. Next Steps: Additional Years & Gold Layer Processing\n",
    "*Review workflow iteration strategy and upcoming gold layer integration*\n",
    "\n",
    "---\n",
    "<br>\n",
    "<details>\n",
    "<summary><strong>Remaining target years</strong> (click to expand)</summary>\n",
    "\n",
    "**Project scope:** Three years for comparative tourism event analysis\n",
    "\n",
    "- **2013:** Super Bowl XLVII (February) - massive tourism influx (40,084 reviews)\n",
    "  \n",
    "- **2016:** Normal tourism baseline - regular seasonal patterns (77,097 reviews)\n",
    "  \n",
    "- **2018:** New Orleans Tricentennial - 300th anniversary celebrations (97,887 reviews)\n",
    "\n",
    "**To process additional years:**\n",
    "\n",
    "1. Update `TARGET_YEAR` variable in Section 6C\n",
    "\n",
    "2. Rerun Sections 6C-8 for each year\n",
    "\n",
    "3. Run Section 9B after all years processed\n",
    "</details>\n",
    "\n",
    "---\n",
    "<br>\n",
    "<details>\n",
    "<summary><strong>Gold layer integration strategy</strong> (click to expand)</summary>\n",
    "\n",
    "**Multi-dataset analysis:** All processed silver datasets (TripAdvisor NYC, Yelp New Orleans, AirBnB LA/Chicago) will be explored for schema compatibility\n",
    "\n",
    "**Reviewer classification:** Three-tier system (locals/frequent visitors/tourists) will be implemented in gold layer using `usr_YYYY_la_rev_count` columns\n",
    "\n",
    "**Data enhancements:**\n",
    "\n",
    "- Null value handling with appropriate strategies\n",
    "\n",
    "- Type conversions for analysis efficiency\n",
    "\n",
    "- Tourism event markers (Super Bowl, Tricentennial, etc.)\n",
    "\n",
    "- Seasonal analysis standardization\n",
    "\n",
    "**Analysis-ready format:** Final gold datasets optimized for sentiment analysis and tourism correlation modeling\n",
    "</details>\n",
    "\n",
    "---\n",
    "<br>\n",
    "<details>\n",
    "<summary><strong>Gold processing pipeline</strong> (click to expand)</summary>\n",
    "\n",
    "**Workflow steps:**\n",
    "\n",
    "1. Load all silver year datasets (2013, 2016, 2018)\n",
    "   \n",
    "2. Implement reviewer classification using annual review patterns\n",
    "   \n",
    "3. Add tourism event context markers\n",
    "   \n",
    "4. Standardize temporal features across datasets\n",
    "   \n",
    "5. Create unified schemas for cross-platform comparison\n",
    "   \n",
    "6. Output analysis-ready gold datasets for sentiment analysis\n",
    "\n",
    "**Integration considerations:**\n",
    "\n",
    "- Schema alignment across TripAdvisor, Yelp, AirBnB datasets\n",
    "\n",
    "- Common temporal features (year, season, event proximity)\n",
    "\n",
    "- Standardized reviewer type classifications\n",
    "\n",
    "- Tourism volume correlation preparation\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceac8051",
   "metadata": {},
   "source": [
    "### 9B. Memory Cleanup & Storage Summary\n",
    "*Clear year selection data and review storage optimization options*\n",
    "\n",
    "**Purpose:** Free memory and identify intermediate files for optional cleanup\n",
    "\n",
    "**When to run:** After processing all target years (2013, 2016, 2018)\n",
    "\n",
    "**Storage optimization:** Review bronze layer intermediate files and cleanup options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8d0d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Memory Cleanup & Storage Summary\")\n",
    "print()\n",
    "\n",
    "# List of variables to clean up\n",
    "cleanup_vars = ['exploration_df', 'year_df', 'year_dist']\n",
    "\n",
    "# Iterate and delete if exists\n",
    "cleanup_count = 0\n",
    "for var in cleanup_vars:\n",
    "    if var in dir():\n",
    "        del globals()[var]\n",
    "        cleanup_count += 1\n",
    "\n",
    "# Force garbage collection\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(f\"✓ Cleaned {cleanup_count} variables from memory\")\n",
    "print()\n",
    "\n",
    "# Show storage breakdown\n",
    "bronze_base = project_root / \"data\" / \"bronze\" / \"yelp\"\n",
    "print_storage_summary(bronze_base, silver_staging, dataset_name=\"Yelp New Orleans\")\n",
    "\n",
    "print()\n",
    "print(\"Workflow complete - ready for gold layer processing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tourism_data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
