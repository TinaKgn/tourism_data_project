{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef4acc6d",
   "metadata": {},
   "source": [
    "# Tourism Sentiment Analysis - TripAdvisor NYC Data Extraction\n",
    "\n",
    "**Project:** Tourism Sentiment Analysis\n",
    "\n",
    "**Task:** Data Extraction & Processing\n",
    "\n",
    "**Dataset Source:** TripAdvisor (SciDB)\n",
    "\n",
    "**Focus:** NYC, 2022-2025, Hotels\n",
    "\n",
    "**Source URL:** https://www.scidb.cn/en/file?fid=df2d477ee4830d106a58c14053a57b07"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b338d0f3",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration\n",
    "*Import libraries, set up project paths, create directory structure*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca49aa0",
   "metadata": {},
   "source": [
    "### 1A. Imports & Script Setup\n",
    "*Load required packages and configure script imports*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d836ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# File handling & web requests\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# System utilities\n",
    "import sys\n",
    "\n",
    "# Bootstrap: Add shared scripts to Python path\n",
    "def setup_scripts_path():\n",
    "    \"\"\"Add shared scripts to Python path\"\"\"\n",
    "    current = Path.cwd()\n",
    "    for _ in range(10):\n",
    "        if (current / '.projectroot').exists():\n",
    "            scripts_dir = current / 'notebooks' / 'shared' / 'scripts'\n",
    "            if scripts_dir.exists():\n",
    "                sys.path.insert(0, str(scripts_dir))\n",
    "                return scripts_dir\n",
    "        if current.parent == current:\n",
    "            break\n",
    "        current = current.parent\n",
    "    raise FileNotFoundError(\n",
    "        \"Scripts directory not found.\\n\"\n",
    "        \"Ensure .projectroot exists at project root.\"\n",
    "    )\n",
    "\n",
    "# Setup path and import utilities\n",
    "scripts_dir = setup_scripts_path()\n",
    "\n",
    "from project_utils import find_project_root\n",
    "from data_io import setup_extraction_directories, check_existing_file, check_existing_chunks\n",
    "from data_validation import print_final_summary, print_storage_summary\n",
    "\n",
    "print(f\"✓ Packages and scripts loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632a5893",
   "metadata": {},
   "source": [
    "### 1B. Project Root Detection\n",
    "*Cross-platform function to locate project directory automatically*\n",
    "\n",
    "**Purpose:** Finds project root by searching for `.projectroot` marker file\n",
    "\n",
    "**Handles:** Working directory issues, different operating systems, various notebook locations\n",
    "\n",
    "**Confirmation:** Displays detected path for verification\n",
    "\n",
    "**Manual Override:** Uncomment line below if auto-detection fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e71cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically detect project root\n",
    "project_root = find_project_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11827e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If auto-detection fails, uncomment and edit the following line:\n",
    "# project_root = Path('/.../.../.../[tourism_data_project]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fe1bdd",
   "metadata": {},
   "source": [
    "### 1C. Set Project Paths\n",
    "*Establish standardized directory structure for bronze and silver processing*\n",
    "\n",
    "**Bronze Structure:** Raw download → chunked conversion → primary filter\n",
    "\n",
    "**Silver Structure:** Final staging area for gold layer integration\n",
    "\n",
    "**Auto-creation:** All directories created automatically for new collaborators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944e3c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all required directories automatically\n",
    "dirs = setup_extraction_directories(project_root, 'tripadvisor')\n",
    "\n",
    "# Access directories throughout notebook\n",
    "original_dir = dirs['bronze_original']\n",
    "conversion_dir = dirs['bronze_conversion']\n",
    "primary_filter_dir = dirs['bronze_primary_filter']\n",
    "silver_staging = dirs['silver_staging']\n",
    "\n",
    "print(\"Directories ready:\")\n",
    "print(f\"  Bronze original: {original_dir}\")\n",
    "print(f\"  Bronze conversion: {conversion_dir}\")\n",
    "print(f\"  Bronze primary filter: {primary_filter_dir}\")\n",
    "print(f\"  Silver staging: {silver_staging}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5010a5fd",
   "metadata": {},
   "source": [
    "## 2. Data Acquisition\n",
    "*Download raw Excel file from ScienceDB using discovered direct API*\n",
    "\n",
    "<details>\n",
    "<summary><strong>Manual Download Instructions </strong> (click to expand)</summary>\n",
    "\n",
    "***If automated download fails:***\n",
    "1. Visit: https://www.scidb.cn/en/file?fid=df2d477ee4830d106a58c14053a57b07\n",
    "2. Download file manually\n",
    "3. Rename to: `tripadvisor_nyc_2022_2025_original.xlsx`\n",
    "4. Place in: `data/bronze/tripadvisor/00_original_download/`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a506b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up download path\n",
    "file_id = \"df2d477ee4830d106a58c14053a57b07\"\n",
    "url = f\"https://china.scidb.cn/download?fileId={file_id}\"\n",
    "file_name = \"tripadvisor_nyc_2022_2025_original.xlsx\"\n",
    "file_path = original_dir / file_name\n",
    "\n",
    "# Check if file already exists\n",
    "exists, info = check_existing_file(file_path, file_type='xlsx', show_info=True)\n",
    "\n",
    "if not exists:\n",
    "    print(f\"Downloading from: {url}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(file_path, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "    file_size = file_path.stat().st_size / (1024 * 1024)\n",
    "    print(f\"Download complete: {file_path}\")\n",
    "    print(f\"File size: {file_size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7169ca5",
   "metadata": {},
   "source": [
    "## 3. Bronze Layer: Raw Data Processing\n",
    "*Convert Excel to chunked parquet files preserving original structure*\n",
    "\n",
    "**Input:** `data/bronze/00_original_download/tripadvisor_nyc_2022_2025_original.xlsx` (156.9 MB)\n",
    "\n",
    "**Output:** `data/bronze/01_raw_conversion/tripadvisor_nyc_raw_chunk_*.parquet` (chunked files)\n",
    "\n",
    "**Processing:** 5,000-row chunks for memory efficiency\n",
    "\n",
    "**Purpose:** Preserve complete dataset structure while converting to analysis-friendly format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e75088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if conversion already completed\n",
    "exists, count = check_existing_chunks(\n",
    "    conversion_dir,\n",
    "    pattern=\"tripadvisor_nyc_raw_chunk_*.parquet\"\n",
    ")\n",
    "\n",
    "if not exists:\n",
    "    print(\"Loading Excel file...\")\n",
    "    wb = load_workbook(file_path, read_only=True)\n",
    "    ws = wb.active\n",
    "    header = [str(cell.value) if cell.value is not None else f\"col_{i}\"\n",
    "              for i, cell in enumerate(next(ws.iter_rows(min_row=1, max_row=1)))]\n",
    "    print(f\"Columns found: {len(header)}\")\n",
    "\n",
    "    # Convert to parquet chunks\n",
    "    chunk_size = 5000\n",
    "    rows = []\n",
    "    part = 0\n",
    "\n",
    "    print(\"Converting to parquet chunks...\")\n",
    "    for row in ws.iter_rows(min_row=2, values_only=True):\n",
    "        row = list(row[:len(header)])  # truncate any extra columns\n",
    "        while len(row) < len(header):  # fill missing columns with None\n",
    "            row.append(None)\n",
    "        rows.append(row)\n",
    "\n",
    "        if len(rows) >= chunk_size:\n",
    "            df = pd.DataFrame(rows, columns=header)\n",
    "            chunk_filename = f\"tripadvisor_nyc_raw_chunk_{part:05d}.parquet\"\n",
    "            pq.write_table(pa.Table.from_pandas(df), conversion_dir / chunk_filename, compression=\"snappy\")\n",
    "            rows = []\n",
    "            part += 1\n",
    "\n",
    "            # Progress indicator every 10 files\n",
    "            if part % 10 == 0:\n",
    "                print(f\"Processed {part} chunks...\")\n",
    "\n",
    "    # Write remaining rows\n",
    "    if rows:\n",
    "        df = pd.DataFrame(rows, columns=header)\n",
    "        chunk_filename = f\"tripadvisor_nyc_raw_chunk_{part:05d}.parquet\"\n",
    "        pq.write_table(pa.Table.from_pandas(df), conversion_dir / chunk_filename, compression=\"snappy\")\n",
    "\n",
    "    print(f\"Conversion complete. Total chunks: {part + 1}\")\n",
    "    print(f\"Output location: {conversion_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99006e46",
   "metadata": {},
   "source": [
    "## 4. Data Verification & Column Inspection\n",
    "*Load converted data to verify structure and examine columns before filtering*\n",
    "\n",
    "**Purpose:** Confirm parquet conversion preserved data integrity\n",
    "\n",
    "**Check:** Column names, data types, row counts\n",
    "\n",
    "**Next:** Identify date column format for primary filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743740c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample chunk to verify conversion\n",
    "sample_file = conversion_dir / \"tripadvisor_nyc_raw_chunk_00000.parquet\"\n",
    "df_sample = pd.read_parquet(sample_file)\n",
    "\n",
    "# Validation checks - flag conversion issues against expected TripAdvisor structure\n",
    "expected_chunk_size = 5000\n",
    "expected_columns = [\n",
    "    'col_0', 'Unnamed: 0', 'hotel_name', 'id_review', 'title',\n",
    "    'date', 'location', 'user_name', 'user_link', 'date_of_stay',\n",
    "    'rating', 'review', 'rating_review', 'n_review_user', 'n_votes_review'\n",
    "]\n",
    "\n",
    "print(f\"Conversion Validation:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. Exact shape validation\n",
    "shape_ok = df_sample.shape == (expected_chunk_size, len(expected_columns))\n",
    "print(f\"{'Shape validated' if shape_ok else 'Shape invalid'}\")\n",
    "print(f\"Shape: {df_sample.shape} [Expected ({expected_chunk_size}, {len(expected_columns)})]\")\n",
    "\n",
    "# 2. Exact column validation\n",
    "columns_ok = list(df_sample.columns) == expected_columns\n",
    "print(f\"{'Columns validated' if columns_ok else 'Columns invalid'}\")\n",
    "print(f\"Column structure: {columns_ok}\")\n",
    "if not columns_ok:\n",
    "    missing = set(expected_columns) - set(df_sample.columns)\n",
    "    extra = set(df_sample.columns) - set(expected_columns)\n",
    "    if missing: print(f\"   Missing: {missing}\")\n",
    "    if extra: print(f\"   Extra: {extra}\")\n",
    "\n",
    "# 3. Sample data display\n",
    "print(f\"\\nSample Data Preview:\")\n",
    "print(df_sample[['hotel_name', 'date', 'rating', 'location']].head(3))\n",
    "\n",
    "# 4. Overall conversion status\n",
    "all_checks_ok = shape_ok and columns_ok\n",
    "print(f\"\\n{'Conversion successful' if all_checks_ok else 'Conversion issues detected'}\")\n",
    "\n",
    "if not all_checks_ok:\n",
    "    print(\"Review validation failures above before proceeding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3e322c",
   "metadata": {},
   "source": [
    "## 5. Primary Filter: Date Range Selection  \n",
    "*Filter reviews to 2022-2025 timeframe and consolidate chunks*\n",
    "\n",
    "**Input:** 84 raw chunks (~500K+ total rows)\n",
    "\n",
    "**Filter Criteria:** Date contains \"2022\", \"2023\", \"2024\", or \"2025\"\n",
    "\n",
    "**Output:** `data/bronze/02_primary_filter/tripadvisor_nyc_2022_2025_date_filtered.parquet`  \n",
    "\n",
    "**Expected Reduction:** ~90% of data (based on original analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9172a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up primary filter output file\n",
    "output_file = primary_filter_dir / \"tripadvisor_nyc_2022_2025_date_filtered.parquet\"\n",
    "\n",
    "# Check if primary filtering already completed\n",
    "exists, info = check_existing_file(output_file, file_type='parquet')\n",
    "\n",
    "if not exists:\n",
    "    # Load all chunks and apply date filter\n",
    "    years_keywords = [\"2022\", \"2023\", \"2024\", \"2025\"]\n",
    "    chunk_files = sorted(conversion_dir.glob(\"tripadvisor_nyc_raw_chunk_*.parquet\"))\n",
    "\n",
    "    print(f\"Processing {len(chunk_files)} chunks for date filtering...\")\n",
    "    all_filtered_rows = []\n",
    "    original_total = 0\n",
    "\n",
    "    for i, chunk_file in enumerate(chunk_files):\n",
    "        df = pd.read_parquet(chunk_file)\n",
    "        original_total += len(df)\n",
    "        date_mask = df[\"date\"].fillna(\"\").apply(lambda x: any(year in str(x) for year in years_keywords))\n",
    "        filtered_df = df[date_mask]\n",
    "        all_filtered_rows.append(filtered_df)\n",
    "\n",
    "        # Progress indicator every 20 files\n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(chunk_files)} chunks...\")\n",
    "\n",
    "    # Consolidate filtered data\n",
    "    print(\"Consolidating filtered chunks...\")\n",
    "    filtered_df = pd.concat(all_filtered_rows, ignore_index=True)\n",
    "\n",
    "    kept_count = len(filtered_df)\n",
    "    removed_count = original_total - kept_count\n",
    "\n",
    "    # Save consolidated result\n",
    "    filtered_df.to_parquet(output_file, compression=\"snappy\")\n",
    "\n",
    "    print(f\"\\nDate filtering complete:\")\n",
    "    print(f\"  Original rows: {original_total:,}\")\n",
    "    print(f\"  Rows kept (2022-2025): {kept_count:,}\")\n",
    "    print(f\"  Rows removed: {removed_count:,}\")\n",
    "    print(f\"  Retention rate: {kept_count/original_total*100:.1f}%\")\n",
    "    print(f\"\\nSaved to: {output_file}\")\n",
    "\n",
    "    # File size check\n",
    "    file_size_mb = output_file.stat().st_size / (1024*1024)\n",
    "    print(f\"File size: {file_size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0293de",
   "metadata": {},
   "source": [
    "## 6. Data Verification & Geographic Filtering\n",
    "*Load converted data to verify structure and extract NYC hotels using positive filtering*\n",
    "\n",
    "**Input:** `data/bronze/02_primary_filter/tripadvisor_nyc_2022_2025_date_filtered.parquet` (48,992 rows)\n",
    "\n",
    "**Strategy:** \n",
    "1. Verify date filtering results\n",
    "2. Positive NYC filtering (hotel names with NYC indicators)\n",
    "3. Manual cleanup of misclassified hotels\n",
    "\n",
    "**Expected Output:** ~12,500 rows, ~125 hotels\n",
    "\n",
    "**Final Location:** `data/silver/tripadvisor/staging/tripadvisor_nyc_2022_2025_final.parquet`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bc7984",
   "metadata": {},
   "source": [
    "### 6A. Data Verification\n",
    "\n",
    "*Load and verify primary filtered data before geographic analysis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5209bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load primary filtered data for geographic analysis\n",
    "primary_filter_file = primary_filter_dir / \"tripadvisor_nyc_2022_2025_date_filtered.parquet\"\n",
    "\n",
    "if not primary_filter_file.exists():\n",
    "    print(\"[Error] Primary filter file not found - run previous steps first\")\n",
    "    print(f\"Expected location: {primary_filter_file}\")\n",
    "else:\n",
    "    exploration_df = pd.read_parquet(primary_filter_file)\n",
    "\n",
    "    print(\"Primary Filter Verification:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"File loaded successfully\")\n",
    "    print(f\"Rows: {len(exploration_df):,}\")\n",
    "    print(f\"Hotels: {exploration_df['hotel_name'].nunique()}\")\n",
    "\n",
    "    # Verify date filtering worked\n",
    "    date_samples = exploration_df['date'].dropna().head(10).tolist()\n",
    "    target_years = [\"2022\", \"2023\", \"2024\", \"2025\"]\n",
    "    dates_valid = any(any(year in str(date) for year in target_years) for date in date_samples)\n",
    "    print(f\"\\n{'Year filter validated' if dates_valid else 'Year filter not valid'}\")\n",
    "    print(f\"Sample dates contain target years: {date_samples[:5]}\")\n",
    "\n",
    "    print(f\"\\nTop 5 hotels by review count:\")\n",
    "    top_hotels = exploration_df['hotel_name'].value_counts().head(5)\n",
    "    for hotel, count in top_hotels.items():\n",
    "        print(f\"  • {hotel}: {count:,} reviews\")\n",
    "\n",
    "    print(f\"\\nReady for geographic filtering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42de744c",
   "metadata": {},
   "source": [
    "### 6B. Exploratory Analysis Section (Optional)\n",
    "\n",
    "**Purpose:** Show analysis process used to develop filtering strategy\n",
    "\n",
    "**Contains:** Implementation strategies for geographic filtering challenges\n",
    "\n",
    "**Note:** *Optional section - can skip to Section 7 for direct save*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b865d11",
   "metadata": {},
   "source": [
    "#### 6B.1 Initial Hotel Name Analysis\n",
    "*Examine hotel name patterns after date filtering*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c59820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load date-filtered data for geographic analysis\n",
    "primary_filter_dir = bronze_base / \"02_primary_filter\"\n",
    "exploration_df = pd.read_parquet(primary_filter_dir / \"tripadvisor_nyc_2022_2025_date_filtered.parquet\")\n",
    "\n",
    "print(f\"Starting geographic analysis with: {len(exploration_df):,} rows\")\n",
    "print(f\"Unique hotels: {exploration_df['hotel_name'].nunique()}\")\n",
    "\n",
    "# Initial hotel name examination\n",
    "print(f\"\\n Top 15 hotels by review count:\")\n",
    "top_hotels = exploration_df['hotel_name'].value_counts().head(15)\n",
    "for hotel, count in top_hotels.items():\n",
    "    print(f\"  • {hotel} ({count:,} reviews)\")\n",
    "\n",
    "# Look for obvious non-NYC patterns\n",
    "print(f\"\\n Sample hotel names (checking for international patterns):\")\n",
    "sample_hotels = exploration_df['hotel_name'].value_counts().head(25).index\n",
    "for hotel in sample_hotels:\n",
    "    print(f\"  • {hotel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2259ed",
   "metadata": {},
   "source": [
    "#### 6B.2 UK Reviewer Concentration Strategy\n",
    "\n",
    "**Approach:** Use reviewer location patterns to identify misclassified hotels*\n",
    "\n",
    "**Challenge:** Hotel names alone insufficient (e.g., \"SoHo\" exists in both NYC and London)\n",
    "\n",
    "**Adjustment:** Analyze reviewer geographic patterns ([user_...] 'location') to detect misclassified hotels\n",
    "\n",
    "**Logic:** London hotels will have high concentrations of UK-based reviewers\n",
    "\n",
    "**Threshold:** Hotels with >60% UK reviewers (min. 10 location entries) flagged for removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e3217b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze reviewer location patterns to identify non-NYC hotels\n",
    "print(\"Analyzing reviewer geographic patterns...\")\n",
    "\n",
    "hotel_stats = []\n",
    "for hotel_name, group in exploration_df.groupby('hotel_name'):\n",
    "    location_data = group['location'].fillna('')\n",
    "\n",
    "    total_reviews = len(group)\n",
    "    total_with_location = group['location'].notna().sum()\n",
    "    uk_reviews = location_data.str.contains('United Kingdom|UK|England|Scotland|Wales', case=False).sum()\n",
    "    shanghai_reviews = location_data.str.contains('Shanghai|China', case=False).sum()\n",
    "\n",
    "    hotel_stats.append({\n",
    "        'hotel_name': hotel_name,\n",
    "        'total_reviews': total_reviews,\n",
    "        'total_with_location': total_with_location,\n",
    "        'uk_reviews': uk_reviews,\n",
    "        'shanghai_reviews': shanghai_reviews\n",
    "    })\n",
    "\n",
    "# Convert to analysis DataFrame\n",
    "hotel_analysis = pd.DataFrame(hotel_stats)\n",
    "hotel_analysis['uk_percentage'] = (hotel_analysis['uk_reviews'] / hotel_analysis['total_with_location']).fillna(0)\n",
    "hotel_analysis['shanghai_percentage'] = (hotel_analysis['shanghai_reviews'] / hotel_analysis['total_with_location']).fillna(0)\n",
    "\n",
    "# Identify problematic hotels\n",
    "uk_threshold = 0.6\n",
    "uk_hotels = hotel_analysis[\n",
    "    (hotel_analysis['uk_percentage'] > uk_threshold) &\n",
    "    (hotel_analysis['total_with_location'] >= 10)\n",
    "]\n",
    "\n",
    "print(f\"\\nHotels with >{uk_threshold*100:.0f}% UK reviewers: {len(uk_hotels)}\")\n",
    "if len(uk_hotels) > 0:\n",
    "    print(\"\\n UK-heavy hotels (likely London):\")\n",
    "    uk_display = uk_hotels.nlargest(10, 'uk_percentage')[['hotel_name', 'total_with_location', 'uk_percentage']]\n",
    "    for _, row in uk_display.iterrows():\n",
    "        print(f\"  • {row['hotel_name']} - {row['uk_percentage']:.1%} UK reviewers ({row['total_with_location']} total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e1fc02",
   "metadata": {},
   "source": [
    "#### 6B.3 Positive NYC Filtering Strategy\n",
    "\n",
    "**Approach:** Identify genuine NYC hotels using location indicators*\n",
    "\n",
    "**Strategy Shift:** Instead of removing non-NYC, actively identify NYC hotels\n",
    "\n",
    "**Indicators:** Hotel names containing NYC-specific terms\n",
    "\n",
    "**Advantage:** Reduces false positives from ambiguous neighborhood names (SoHo, Chelsea, etc.)\n",
    "\n",
    "**Final Cleanup:** Manual removal of remaining misclassified hotels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fd92fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply positive NYC filtering - identify genuine NYC hotels\n",
    "nyc_indicators = [\n",
    "    'New York', 'NYC', 'Manhattan', 'Brooklyn', 'Queens', 'Bronx',\n",
    "    'Times Square', 'Time Square', 'Central Park', 'Wall Street',\n",
    "    'Midtown', 'Downtown', 'Financial District', 'SoHo', 'NoMad',\n",
    "    'TriBeCa', 'Upper East', 'Upper West', 'Lower East', 'Herald Square',\n",
    "    'Penn Station', 'Grand Central', 'JFK', 'LaGuardia', 'Empire State'\n",
    "]\n",
    "\n",
    "nyc_pattern = '|'.join(nyc_indicators)\n",
    "nyc_hotels = exploration_df[exploration_df['hotel_name'].str.contains(nyc_pattern, case=False, na=False)]\n",
    "\n",
    "print(f\"NYC hotels identified: {len(nyc_hotels):,} rows\")\n",
    "print(f\"Unique NYC hotels: {nyc_hotels['hotel_name'].nunique()}\")\n",
    "\n",
    "# Check for remaining ambiguous terms that might be misclassified\n",
    "print(f\"\\n Top 10 NYC hotels:\")\n",
    "nyc_top = nyc_hotels['hotel_name'].value_counts().head(10)\n",
    "for hotel, count in nyc_top.items():\n",
    "    print(f\"  • {hotel} ({count:,} reviews)\")\n",
    "\n",
    "# Check for potentially ambiguous hotels needing manual review\n",
    "ambiguous_terms = ['SoHo', 'Chelsea', 'Greenwich', 'Victoria']\n",
    "print(f\"\\n NYC hotels with ambiguous neighborhood terms\")\n",
    "for term in ambiguous_terms:\n",
    "    matching = nyc_hotels[nyc_hotels['hotel_name'].str.contains(term, case=False, na=False)]\n",
    "    if len(matching) > 0:\n",
    "        unique_hotels = matching['hotel_name'].unique()\n",
    "        print(f\"\\n  {term}: {len(unique_hotels)} hotels\")\n",
    "        for hotel in unique_hotels[:3]:\n",
    "            print(f\"    • {hotel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a2ecce",
   "metadata": {},
   "source": [
    "#### 6B.4 Manual Cleanup of Misclassified Hotels\n",
    "*Remove remaining London hotels caught by ambiguous neighborhood names*\n",
    "\n",
    "**Issue:** \"SoHo\" exists in both NYC and London\n",
    "\n",
    "**Solution:** Remove clearly London-branded hotels\n",
    "\n",
    "**Targets:** Hotels with \"London\" in name or known London hotel chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1687d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual removal of identified London hotels\n",
    "london_hotels_to_remove = [\n",
    "    'The Soho Hotel',                       # London SoHo hotel\n",
    "    'The Z Hotel Soho',                     # London hotel chain\n",
    "    'hub by Premier Inn London Soho hotel'  # Explicitly London-branded\n",
    "]\n",
    "\n",
    "print(f\"Removing London hotels:\")\n",
    "for hotel in london_hotels_to_remove:\n",
    "    count = nyc_hotels[nyc_hotels['hotel_name'] == hotel].shape[0]\n",
    "    print(f\"  • {hotel} ({count:,} reviews)\")\n",
    "\n",
    "# Apply manual cleanup\n",
    "final_nyc_df = nyc_hotels[~nyc_hotels['hotel_name'].isin(london_hotels_to_remove)].copy()\n",
    "\n",
    "print(f\"\\nManual cleanup complete...\")\n",
    "print(f\"Final NYC dataset: {len(final_nyc_df):,} rows\")\n",
    "print(f\"Unique hotels: {final_nyc_df['hotel_name'].nunique()}\")\n",
    "\n",
    "# Verify remaining SoHo hotels are legitimate NYC hotels\n",
    "remaining_soho = final_nyc_df[final_nyc_df['hotel_name'].str.contains('soho', case=False)]['hotel_name'].unique()\n",
    "print(f\"\\nRemaining SoHo hotels (verified NYC):\")\n",
    "for hotel in remaining_soho:\n",
    "    print(f\"  • {hotel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c964eae",
   "metadata": {},
   "source": [
    "## 7. Silver Layer: Final Geographic Filter & Save\n",
    "*Clean, validated approach - works whether exploration was run or not*\n",
    "\n",
    "**Implementation:** Apply proven NYC filter strategy\n",
    "\n",
    "**Output:** `data/silver/tripadvisor/staging/tripadvisor_nyc_2022_2025_final.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86477de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up silver staging output file\n",
    "output_file = silver_staging / \"tripadvisor_nyc_2022_2025_final.parquet\"\n",
    "\n",
    "# Check if final filtering already completed\n",
    "exists, info = check_existing_file(output_file, file_type='parquet')\n",
    "\n",
    "if not exists:\n",
    "    # Load primary filtered data (works whether exploration was run or skipped)\n",
    "    df_for_filtering = pd.read_parquet(primary_filter_dir / \"tripadvisor_nyc_2022_2025_date_filtered.parquet\")\n",
    "\n",
    "    # Apply validated NYC filter strategy\n",
    "    nyc_indicators = [\n",
    "        'New York', 'NYC', 'Manhattan', 'Brooklyn', 'Queens', 'Bronx',\n",
    "        'Times Square', 'Time Square', 'Central Park', 'Wall Street',\n",
    "        'Midtown', 'Downtown', 'Financial District', 'SoHo', 'NoMad',\n",
    "        'TriBeCa', 'Upper East', 'Upper West', 'Lower East', 'Herald Square',\n",
    "        'Penn Station', 'Grand Central', 'JFK', 'LaGuardia', 'Empire State'\n",
    "    ]\n",
    "\n",
    "    nyc_pattern = '|'.join(nyc_indicators)\n",
    "    nyc_filtered = df_for_filtering[df_for_filtering['hotel_name'].str.contains(nyc_pattern, case=False, na=False)]\n",
    "\n",
    "    # Remove identified London hotels\n",
    "    london_hotels_to_remove = ['The Soho Hotel', 'The Z Hotel Soho', 'hub by Premier Inn London Soho hotel']\n",
    "    final_clean_df = nyc_filtered[~nyc_filtered['hotel_name'].isin(london_hotels_to_remove)].copy()\n",
    "\n",
    "    # Save to silver staging directory\n",
    "    final_clean_df.to_parquet(output_file, compression=\"snappy\")\n",
    "\n",
    "    print(f\"Final dataset saved\")\n",
    "    print(f\"Location: {output_file}\")\n",
    "    file_size_mb = output_file.stat().st_size / (1024*1024)\n",
    "    print(f\"Rows: {len(final_clean_df):,}\")\n",
    "    print(f\"Hotels: {final_clean_df['hotel_name'].nunique()}\")\n",
    "    print(f\"File size: {file_size_mb:.1f} MB\")\n",
    "    print(f\"\\nReady for gold layer processing and analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3ee782",
   "metadata": {},
   "source": [
    "## 8. Final Verification & Cleanup\n",
    "\n",
    "*Verify saved dataset and optional cleanup of intermediate files*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a1bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comprehensive final summary\n",
    "print_final_summary(\n",
    "    output_file,\n",
    "    dataset_name=\"TripAdvisor NYC 2022-2025\",\n",
    "    file_type='parquet'\n",
    ")\n",
    "\n",
    "# Print storage breakdown\n",
    "bronze_base = project_root / \"data\" / \"bronze\" / \"tripadvisor\"\n",
    "print_storage_summary(\n",
    "    bronze_base,\n",
    "    silver_staging,\n",
    "    dataset_name=\"TripAdvisor NYC\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c64361",
   "metadata": {},
   "source": [
    "## 9. Next Steps: Gold Layer Processing\n",
    "\n",
    "**Current Status:**\n",
    "Bronze → Silver workflow complete for TripAdvisor NYC dataset\n",
    "\n",
    "**Upcoming Gold Layer Integration:**\n",
    "\n",
    "- **Multi-dataset analysis:** All processed silver datasets (TripAdvisor NYC, Yelp New Orleans, AirBnB LA/Chicago) will be explored for shared columns\n",
    "  \n",
    "- **Schema standardization:** Common fields (location, date, rating, text) will be unified across datasets\n",
    "  \n",
    "- **Data quality:** Null value handling, strategic imputation, and appropriate data type conversions\n",
    "  \n",
    "- **Analysis-ready format:** Final gold datasets optimized for sentiment analysis and tourism correlation modeling\n",
    "\n",
    "**Gold Processing Pipeline:**\n",
    "1. Load all silver datasets and analyze column overlap\n",
    "2. Standardize shared column names and formats\n",
    "3. Handle missing values with dataset-appropriate strategies\n",
    "4. Convert data types for analysis efficiency\n",
    "5. Create unified gold datasets for cross-platform analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e08bf6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tourism_data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
