{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef4acc6d",
   "metadata": {},
   "source": [
    "# Tourism Sentiment Analysis - TripAdvisor NYC Data Extraction\n",
    "\n",
    "**Project:** Tourism Sentiment Analysis\n",
    "\n",
    "**Task:** Data Extraction & Processing\n",
    "\n",
    "**Dataset Source:** TripAdvisor (SciDB)\n",
    "\n",
    "**Focus:** NYC, 2022-2025, Hotels\n",
    "\n",
    "**Source URL:** https://www.scidb.cn/en/file?fid=df2d477ee4830d106a58c14053a57b07"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b338d0f3",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration\n",
    "*Import libraries, set up project paths, create directory structure*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca49aa0",
   "metadata": {},
   "source": [
    "### 1A. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0921703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632a5893",
   "metadata": {},
   "source": [
    "### 1B. Project Root Detection\n",
    "*Cross-platform function to locate project directory automatically*\n",
    "\n",
    "**Purpose:** Finds project root by searching for `.python-version` and `.gitignore` files\n",
    "\n",
    "**Handles:** Working directory issues, different operating systems, various notebook locations\n",
    "\n",
    "**Fallback:** Provides clear error message with troubleshooting steps if project root not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "726e71cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_project_root():\n",
    "    \"\"\"Find project root - flexible and robust\"\"\"\n",
    "    from pathlib import Path\n",
    "    import os\n",
    "    # Strategy 1: Look for tourism project indicators\n",
    "    current = Path.cwd()\n",
    "    # Search up the directory tree for project indicators\n",
    "    for _ in range(10):\n",
    "        # Check for any tourism project signs\n",
    "        tourism_indicators = [\n",
    "            current.name.lower().find('tourism') != -1,\n",
    "            (current / \"notebooks\").exists(),\n",
    "            (current / \"data\").exists(),\n",
    "            any(f.name.endswith('.ipynb') for f in current.glob('*') if f.is_file())\n",
    "        ]\n",
    "        if any(tourism_indicators):\n",
    "            # Create marker files if missing\n",
    "            (current / \".python-version\").touch(exist_ok=True)\n",
    "            (current / \".gitignore\").touch(exist_ok=True)\n",
    "            return current\n",
    "        if current.parent == current:  # Reached filesystem root\n",
    "            break\n",
    "        current = current.parent\n",
    "    # Fallback: Use current working directory\n",
    "    project_root = Path.cwd()\n",
    "    # Ensure marker files exist\n",
    "    (project_root / \".python-version\").touch(exist_ok=True)\n",
    "    (project_root / \".gitignore\").touch(exist_ok=True)\n",
    "    return project_root\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fe1bdd",
   "metadata": {},
   "source": [
    "### 1C. Set Project Paths\n",
    "*Establish standardized directory structure for bronze and silver processing*\n",
    "\n",
    "**Bronze Structure:** Raw download → chunked conversion → primary filter\n",
    "\n",
    "**Silver Structure:** Final staging area for gold layer integration\n",
    "\n",
    "**Auto-creation:** All directories created automatically for new collaborators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "944e3c08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/anna/code/TinaKgn/tourism_data_project/notebooks/shared/data_extraction/tripadvisor\n",
      "Bronze base: /home/anna/code/TinaKgn/tourism_data_project/notebooks/shared/data_extraction/tripadvisor/data/bronze/tripadvisor\n"
     ]
    }
   ],
   "source": [
    "# Set up project paths with bronze subfolder structure\n",
    "project_root = find_project_root()\n",
    "bronze_base = project_root / \"data\" / \"bronze\" / \"tripadvisor\"\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Bronze base: {bronze_base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5010a5fd",
   "metadata": {},
   "source": [
    "## 2. Data Acquisition\n",
    "*Download raw Excel file from ScienceDB using discovered direct API*\n",
    "\n",
    "<details>\n",
    "<summary><strong>Manual Download Instructions </strong> (click to expand)</summary>\n",
    "\n",
    "***If automated download fails:***\n",
    "1. Visit: https://www.scidb.cn/en/file?fid=df2d477ee4830d106a58c14053a57b07\n",
    "2. Download file manually\n",
    "3. Rename to: `tripadvisor_nyc_2022_2025_original.xlsx`\n",
    "4. Place in: `data/bronze/tripadvisor/00_original_download/`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a506b8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from: https://china.scidb.cn/download?fileId=df2d477ee4830d106a58c14053a57b07...\n",
      "Download complete: /home/anna/code/TinaKgn/tourism_data_project/notebooks/shared/data_extraction/tripadvisor/data/bronze/tripadvisor/00_original_download/tripadvisor_nyc_2022_2025_original.xlsx\n",
      "File size: 156.9 MB\n"
     ]
    }
   ],
   "source": [
    "# Set up download directory\n",
    "original_dir = bronze_base / \"00_original_download\"\n",
    "original_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Direct download URL (SciDB.cn pattern)\n",
    "file_id = \"df2d477ee4830d106a58c14053a57b07\"\n",
    "url = f\"https://china.scidb.cn/download?fileId={file_id}\"\n",
    "file_name = \"tripadvisor_nyc_2022_2025_original.xlsx\"\n",
    "file_path = original_dir / file_name\n",
    "\n",
    "# Download the file\n",
    "if not file_path.exists():\n",
    "    print(f\"Downloading from: {url}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(file_path, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    print(f\"Download complete: {file_path}\")\n",
    "else:\n",
    "    print(f\"File already exists: {file_path}\")\n",
    "\n",
    "# Check file size\n",
    "file_size = file_path.stat().st_size / (1024 * 1024)\n",
    "print(f\"File size: {file_size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7169ca5",
   "metadata": {},
   "source": [
    "## 3. Bronze Layer: Raw Data Processing\n",
    "*Convert Excel to chunked parquet files preserving original structure*\n",
    "\n",
    "**Input:** `data/bronze/00_original_download/tripadvisor_nyc_2022_2025_original.xlsx` (156.9 MB)\n",
    "\n",
    "**Output:** `data/bronze/01_raw_conversion/tripadvisor_nyc_raw_chunk_*.parquet` (chunked files)\n",
    "\n",
    "**Processing:** 5,000-row chunks for memory efficiency\n",
    "\n",
    "**Purpose:** Preserve complete dataset structure while converting to analysis-friendly format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e75088e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Excel file...\n",
      "Columns found: 15\n",
      "Converting to parquet chunks...\n",
      "Processed 10 chunks...\n",
      "Processed 20 chunks...\n",
      "Processed 30 chunks...\n",
      "Processed 40 chunks...\n",
      "Processed 50 chunks...\n",
      "Processed 60 chunks...\n",
      "Processed 70 chunks...\n",
      "Processed 80 chunks...\n",
      "Conversion complete. Total chunks: 84\n",
      "Output location: /home/anna/code/TinaKgn/tourism_data_project/notebooks/shared/data_extraction/tripadvisor/data/bronze/tripadvisor/01_raw_conversion\n"
     ]
    }
   ],
   "source": [
    "# Set up conversion output directory\n",
    "conversion_dir = bronze_base / \"01_raw_conversion\"\n",
    "conversion_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check if conversion already completed\n",
    "existing_chunks = list(conversion_dir.glob(\"tripadvisor_nyc_raw_chunk_*.parquet\"))\n",
    "if existing_chunks:\n",
    "    print(f\"[SKIP] Conversion already complete - found {len(existing_chunks)} existing chunks\")\n",
    "    print(f\"Output location: {conversion_dir}\")\n",
    "    print(\"\\nTo reconvert, delete the 01_raw_conversion folder first\")\n",
    "else:\n",
    "    print(\"Loading Excel file...\")\n",
    "    wb = load_workbook(file_path, read_only=True)\n",
    "    ws = wb.active\n",
    "    header = [str(cell.value) if cell.value is not None else f\"col_{i}\" for i, cell in enumerate(next(ws.iter_rows(min_row=1, max_row=1)))]\n",
    "    print(f\"Columns found: {len(header)}\")\n",
    "\n",
    "    # Convert to parquet chunks\n",
    "    chunk_size = 5000\n",
    "    rows = []\n",
    "    part = 0\n",
    "\n",
    "    print(\"Converting to parquet chunks...\")\n",
    "    for row in ws.iter_rows(min_row=2, values_only=True):\n",
    "        row = list(row[:len(header)])  # truncate any extra columns\n",
    "        while len(row) < len(header):  # fill missing columns with None\n",
    "            row.append(None)\n",
    "        rows.append(row)\n",
    "\n",
    "        if len(rows) >= chunk_size:\n",
    "            df = pd.DataFrame(rows, columns=header)\n",
    "            chunk_filename = f\"tripadvisor_nyc_raw_chunk_{part:05d}.parquet\"\n",
    "            pq.write_table(pa.Table.from_pandas(df), conversion_dir / chunk_filename, compression=\"snappy\")\n",
    "            rows = []\n",
    "            part += 1\n",
    "\n",
    "            # Progress indicator every 10 files\n",
    "            if part % 10 == 0:\n",
    "                print(f\"Processed {part} chunks...\")\n",
    "\n",
    "    # Write remaining rows\n",
    "    if rows:\n",
    "        df = pd.DataFrame(rows, columns=header)\n",
    "        chunk_filename = f\"tripadvisor_nyc_raw_chunk_{part:05d}.parquet\"\n",
    "        pq.write_table(pa.Table.from_pandas(df), conversion_dir / chunk_filename, compression=\"snappy\")\n",
    "\n",
    "    print(f\"Conversion complete. Total chunks: {part + 1}\")\n",
    "    print(f\"Output location: {conversion_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99006e46",
   "metadata": {},
   "source": [
    "## 4. Data Verification & Column Inspection\n",
    "*Load converted data to verify structure and examine columns before filtering*\n",
    "\n",
    "**Purpose:** Confirm parquet conversion preserved data integrity\n",
    "\n",
    "**Check:** Column names, data types, row counts\n",
    "\n",
    "**Next:** Identify date column format for primary filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "743740c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion Validation:\n",
      "========================================\n",
      "Shape validated\n",
      "Shape:(5000, 15) [Expected (5000, 15)])\n",
      "Columns validated\n",
      "Column structure: True\n",
      "\n",
      "Sample Data Preview:\n",
      "                         hotel_name      date  rating  \\\n",
      "0  Premier Inn London Holborn hotel     Feb 3      30   \n",
      "1  Premier Inn London Holborn hotel  Mar 2022      50   \n",
      "2  Premier Inn London Holborn hotel  Jan 2023      50   \n",
      "\n",
      "                      location  \n",
      "0                         None  \n",
      "1  Bournemouth, United Kingdom  \n",
      "2  Cleckheaton, United Kingdom  \n",
      "\n",
      "Conversion Successful...\n"
     ]
    }
   ],
   "source": [
    "# Load a sample chunk to verify conversion\n",
    "sample_file = conversion_dir / \"tripadvisor_nyc_raw_chunk_00000.parquet\"\n",
    "df_sample = pd.read_parquet(sample_file)\n",
    "\n",
    "# Validation checks - flag conversion issues against expected TripAdvisor structure\n",
    "expected_chunk_size = 5000\n",
    "expected_columns = [\n",
    "    'col_0', 'Unnamed: 0', 'hotel_name', 'id_review', 'title',\n",
    "    'date', 'location', 'user_name', 'user_link', 'date_of_stay',\n",
    "    'rating', 'review', 'rating_review', 'n_review_user', 'n_votes_review'\n",
    "]\n",
    "\n",
    "print(f\"Conversion Validation:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. Exact shape validation\n",
    "shape_ok = df_sample.shape == (expected_chunk_size, len(expected_columns))\n",
    "print(f\"{'Shape validated' if shape_ok else 'Shape invalid'}\\nShape:{df_sample.shape} [Expected ({expected_chunk_size}, {len(expected_columns)})])\")\n",
    "\n",
    "# 2. Exact column validation\n",
    "columns_ok = list(df_sample.columns) == expected_columns\n",
    "print(f\"{'Columns validated' if columns_ok else 'Columns invalid'}\\nColumn structure: {columns_ok}\")\n",
    "if not columns_ok:\n",
    "    missing = set(expected_columns) - set(df_sample.columns)\n",
    "    extra = set(df_sample.columns) - set(expected_columns)\n",
    "    if missing: print(f\"   Missing: {missing}\")\n",
    "    if extra: print(f\"   Extra: {extra}\")\n",
    "\n",
    "# 3. Sample data display\n",
    "print(f\"\\nSample Data Preview:\")\n",
    "print(df_sample[['hotel_name', 'date', 'rating', 'location']].head(3))\n",
    "\n",
    "# 4. Overall conversion status\n",
    "all_checks_ok = shape_ok and columns_ok\n",
    "print(f\"\\n{'Conversion Successful...' if all_checks_ok else 'Conversion Issues Detected...'}\")\n",
    "\n",
    "if not all_checks_ok:\n",
    "    print(\"Review validation failures above before proceeding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3e322c",
   "metadata": {},
   "source": [
    "## 5. Primary Filter: Date Range Selection  \n",
    "*Filter reviews to 2022-2025 timeframe and consolidate chunks*\n",
    "\n",
    "**Input:** 84 raw chunks (~500K+ total rows)\n",
    "\n",
    "**Filter Criteria:** Date contains \"2022\", \"2023\", \"2024\", or \"2025\"\n",
    "\n",
    "**Output:** `data/bronze/02_primary_filter/tripadvisor_nyc_2022_2025_date_filtered.parquet`  \n",
    "\n",
    "**Expected Reduction:** ~90% of data (based on original analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9172a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 84 chunks for date filtering...\n",
      "Processed 20/84 chunks...\n",
      "Processed 40/84 chunks...\n",
      "Processed 60/84 chunks...\n",
      "Processed 80/84 chunks...\n",
      "Consolidating filtered chunks...\n",
      "\n",
      "Date filtering complete:\n",
      "  Original rows: 416,032\n",
      "  Rows kept (2022-2025): 48,992\n",
      "  Rows removed: 367,040\n",
      "  Retention rate: 11.8%\n",
      "\n",
      "Saved to: /home/anna/code/TinaKgn/tourism_data_project/notebooks/shared/data_extraction/tripadvisor/data/bronze/tripadvisor/02_primary_filter/tripadvisor_nyc_2022_2025_date_filtered.parquet\n",
      "File size: 17.7 MB\n"
     ]
    }
   ],
   "source": [
    "# Set up primary filter output directory\n",
    "primary_filter_dir = bronze_base / \"02_primary_filter\"\n",
    "primary_filter_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check if primary filtering already completed\n",
    "output_file = primary_filter_dir / \"tripadvisor_nyc_2022_2025_date_filtered.parquet\"\n",
    "\n",
    "if output_file.exists():\n",
    "    # Load existing file to show stats\n",
    "    existing_df = pd.read_parquet(output_file)\n",
    "    file_size_mb = output_file.stat().st_size / (1024*1024)\n",
    "    print(f\"[SKIP] Primary filter already complete\")\n",
    "    print(f\"Existing file: {len(existing_df):,} rows, {file_size_mb:.1f} MB\")\n",
    "    print(f\"Location: {output_file}\")\n",
    "    print(\"\\nTo reprocess, delete the 02_primary_filter folder first\")\n",
    "else:\n",
    "    # Load all chunks and apply date filter\n",
    "    years_keywords = [\"2022\", \"2023\", \"2024\", \"2025\"]\n",
    "    chunk_files = sorted(conversion_dir.glob(\"tripadvisor_nyc_raw_chunk_*.parquet\"))\n",
    "\n",
    "    print(f\"Processing {len(chunk_files)} chunks for date filtering...\")\n",
    "    all_filtered_rows = []\n",
    "    original_total = 0\n",
    "\n",
    "    for i, chunk_file in enumerate(chunk_files):\n",
    "        df = pd.read_parquet(chunk_file)\n",
    "        original_total += len(df)\n",
    "        date_mask = df[\"date\"].fillna(\"\").apply(lambda x: any(year in str(x) for year in years_keywords))\n",
    "        filtered_df = df[date_mask]\n",
    "        all_filtered_rows.append(filtered_df)\n",
    "\n",
    "        # Progress indicator every 20 files\n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(chunk_files)} chunks...\")\n",
    "\n",
    "    # Consolidate filtered data\n",
    "    print(\"Consolidating filtered chunks...\")\n",
    "    filtered_df = pd.concat(all_filtered_rows, ignore_index=True)\n",
    "\n",
    "    kept_count = len(filtered_df)\n",
    "    removed_count = original_total - kept_count\n",
    "\n",
    "    # Save consolidated result\n",
    "    filtered_df.to_parquet(output_file, compression=\"snappy\")\n",
    "\n",
    "    print(f\"\\nDate filtering complete:\")\n",
    "    print(f\"  Original rows: {original_total:,}\")\n",
    "    print(f\"  Rows kept (2022-2025): {kept_count:,}\")\n",
    "    print(f\"  Rows removed: {removed_count:,}\")\n",
    "    print(f\"  Retention rate: {kept_count/original_total*100:.1f}%\")\n",
    "    print(f\"\\nSaved to: {output_file}\")\n",
    "\n",
    "    # File size check\n",
    "    file_size_mb = output_file.stat().st_size / (1024*1024)\n",
    "    print(f\"File size: {file_size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0293de",
   "metadata": {},
   "source": [
    "## 6. Data Verification & Geographic Filtering\n",
    "*Load converted data to verify structure and extract NYC hotels using positive filtering*\n",
    "\n",
    "**Input:** `data/bronze/02_primary_filter/tripadvisor_nyc_2022_2025_date_filtered.parquet` (48,992 rows)\n",
    "\n",
    "**Strategy:** \n",
    "1. Verify date filtering results\n",
    "2. Positive NYC filtering (hotel names with NYC indicators)\n",
    "3. Manual cleanup of misclassified hotels\n",
    "\n",
    "**Expected Output:** ~12,500 rows, ~125 hotels\n",
    "\n",
    "**Final Location:** `data/silver/tripadvisor/staging/tripadvisor_nyc_2022_2025_final.parquet`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bc7984",
   "metadata": {},
   "source": [
    "### 6A. Data Verification\n",
    "\n",
    "*Load and verify primary filtered data before geographic analysis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc5209bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary Filter Verification:\n",
      "========================================\n",
      "File loaded successfully\n",
      "Rows: 48,992\n",
      "Hotels: 668\n",
      "\n",
      "Year filter validated: \n",
      "Sample dates contain target years: ['Mar 2022', 'Jan 2023', 'Jan 2023', 'Jan 2023', 'Jan 2023']\n",
      "\n",
      "Top 5 hotels by review count:\n",
      "  • Park Plaza Westminster Bridge London: 749 reviews\n",
      "  • Luma Hotel Time Square: 712 reviews\n",
      "  • The Clermont, Charing Cross: 660 reviews\n",
      "  • Travelodge London City hotel: 616 reviews\n",
      "  • Hyatt Grand Central New York: 595 reviews\n",
      "\n",
      "Ready for geographic filtering\n"
     ]
    }
   ],
   "source": [
    "# Load primary filtered data for geographic analysis\n",
    "primary_filter_file = bronze_base / \"02_primary_filter\" / \"tripadvisor_nyc_2022_2025_date_filtered.parquet\"\n",
    "\n",
    "if not primary_filter_file.exists():\n",
    "    print(\"[ERROR] Primary filter file not found - run previous steps first\")\n",
    "    print(f\"Expected location: {primary_filter_file}\")\n",
    "else:\n",
    "    exploration_df = pd.read_parquet(primary_filter_file)\n",
    "\n",
    "    print(\"Primary Filter Verification:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"File loaded successfully\")\n",
    "    print(f\"Rows: {len(exploration_df):,}\")\n",
    "    print(f\"Hotels: {exploration_df['hotel_name'].nunique()}\")\n",
    "\n",
    "    # Verify date filtering worked\n",
    "    date_samples = exploration_df['date'].dropna().head(10).tolist()\n",
    "    target_years = [\"2022\", \"2023\", \"2024\", \"2025\"]\n",
    "    dates_valid = any(any(year in str(date) for year in target_years) for date in date_samples)\n",
    "    print(f\"\\n{'Year filter validated' if dates_valid else 'Year filter not valid'}: \\nSample dates contain target years: {date_samples[:5]}\")\n",
    "\n",
    "    print(f\"\\nTop 5 hotels by review count:\")\n",
    "    top_hotels = exploration_df['hotel_name'].value_counts().head(5)\n",
    "    for hotel, count in top_hotels.items():\n",
    "        print(f\"  • {hotel}: {count:,} reviews\")\n",
    "\n",
    "    print(f\"\\nReady for geographic filtering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42de744c",
   "metadata": {},
   "source": [
    "### 6B. Exploratory Analysis Section (Optional)\n",
    "\n",
    "**Purpose:** Show analysis process used to develop filtering strategy\n",
    "\n",
    "***Status:*** *Optional - Jump to \"7. Final Geographic Filter & Save\" to run workflow*\n",
    "\n",
    "**Contains:** Implementation strategies for geographic filtering challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b865d11",
   "metadata": {},
   "source": [
    "#### 6B.1 Initial Hotel Name Analysis\n",
    "*Examine hotel name patterns after date filtering*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6c59820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting geographic analysis with: 48,992 rows\n",
      "Unique hotels: 668\n",
      "\n",
      " Top 15 hotels by review count:\n",
      "  • Park Plaza Westminster Bridge London (749 reviews)\n",
      "  • Luma Hotel Time Square (712 reviews)\n",
      "  • The Clermont, Charing Cross (660 reviews)\n",
      "  • Travelodge London City hotel (616 reviews)\n",
      "  • Hyatt Grand Central New York (595 reviews)\n",
      "  • Sea Containers London (465 reviews)\n",
      "  • Travelodge London Central Waterloo (449 reviews)\n",
      "  • Travelodge London Greenwich High Road (437 reviews)\n",
      "  • Hyatt Centric Times Square New York (434 reviews)\n",
      "  • Hotel Riu Plaza New York Times Square (431 reviews)\n",
      "  • The Resident Covent Garden (431 reviews)\n",
      "  • Leonardo Royal London Tower Bridge (418 reviews)\n",
      "  • Travelodge London Central City Road (409 reviews)\n",
      "  • Park Grand London Hyde Park (402 reviews)\n",
      "  • Travelodge London Farringdon (390 reviews)\n",
      "\n",
      " Sample hotel names (checking for international patterns):\n",
      "  • Park Plaza Westminster Bridge London\n",
      "  • Luma Hotel Time Square\n",
      "  • The Clermont, Charing Cross\n",
      "  • Travelodge London City hotel\n",
      "  • Hyatt Grand Central New York\n",
      "  • Sea Containers London\n",
      "  • Travelodge London Central Waterloo\n",
      "  • Travelodge London Greenwich High Road\n",
      "  • Hyatt Centric Times Square New York\n",
      "  • Hotel Riu Plaza New York Times Square\n",
      "  • The Resident Covent Garden\n",
      "  • Leonardo Royal London Tower Bridge\n",
      "  • Travelodge London Central City Road\n",
      "  • Park Grand London Hyde Park\n",
      "  • Travelodge London Farringdon\n",
      "  • Premier Inn London St Pancras hotel\n",
      "  • Travelodge London Stratford\n",
      "  • Park Plaza London Waterloo\n",
      "  • Travelodge London Tower Bridge\n",
      "  • Millennium Gloucester Hotel London Kensington\n",
      "  • Premier Inn London King's Cross Hotel\n",
      "  • DoubleTree by Hilton Hotel New York Times Square West\n",
      "  • The Savoy\n",
      "  • Hyatt Place New York/Chelsea\n",
      "  • Canopy by Hilton London City\n"
     ]
    }
   ],
   "source": [
    "# Load date-filtered data for geographic analysis\n",
    "primary_filter_dir = bronze_base / \"02_primary_filter\"\n",
    "exploration_df = pd.read_parquet(primary_filter_dir / \"tripadvisor_nyc_2022_2025_date_filtered.parquet\")\n",
    "\n",
    "print(f\"Starting geographic analysis with: {len(exploration_df):,} rows\")\n",
    "print(f\"Unique hotels: {exploration_df['hotel_name'].nunique()}\")\n",
    "\n",
    "# Initial hotel name examination\n",
    "print(f\"\\n Top 15 hotels by review count:\")\n",
    "top_hotels = exploration_df['hotel_name'].value_counts().head(15)\n",
    "for hotel, count in top_hotels.items():\n",
    "    print(f\"  • {hotel} ({count:,} reviews)\")\n",
    "\n",
    "# Look for obvious non-NYC patterns\n",
    "print(f\"\\n Sample hotel names (checking for international patterns):\")\n",
    "sample_hotels = exploration_df['hotel_name'].value_counts().head(25).index\n",
    "for hotel in sample_hotels:\n",
    "    print(f\"  • {hotel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2259ed",
   "metadata": {},
   "source": [
    "#### 6B.2 UK Reviewer Concentration Strategy\n",
    "\n",
    "**Approach:** Use reviewer location patterns to identify misclassified hotels*\n",
    "\n",
    "**Challenge:** Hotel names alone insufficient (e.g., \"SoHo\" exists in both NYC and London)\n",
    "\n",
    "**Adjustment:** Analyze reviewer geographic patterns ([user_...] 'location') to detect misclassified hotels\n",
    "\n",
    "**Logic:** London hotels will have high concentrations of UK-based reviewers\n",
    "\n",
    "**Threshold:** Hotels with >60% UK reviewers (min. 10 location entries) flagged for removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29e3217b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing reviewer geographic patterns...\n",
      "\n",
      "Hotels with >60% UK reviewers: 141\n",
      "\n",
      " UK-heavy hotels (likely London):\n",
      "  • Premier Inn London Hanger Lane hotel - 92.7% UK reviewers (96 total)\n",
      "  • Fitzrovia Hotel - 90.5% UK reviewers (21 total)\n",
      "  • Premier Inn London New Southgate Hotel - 90.5% UK reviewers (21 total)\n",
      "  • The Chamberlain Hotel - 90.0% UK reviewers (30 total)\n",
      "  • Premier Inn London Archway hotel - 88.1% UK reviewers (109 total)\n",
      "  • The Luxury Inn - 87.5% UK reviewers (16 total)\n",
      "  • hub by Premier Inn London Spitalfields, Brick Lane hotel - 85.9% UK reviewers (85 total)\n",
      "  • Premier Inn London Greenwich hotel - 85.5% UK reviewers (124 total)\n",
      "  • Premier Inn London Tolworth - 85.5% UK reviewers (62 total)\n",
      "  • The Prince of Wales - Townhouse - 84.6% UK reviewers (13 total)\n"
     ]
    }
   ],
   "source": [
    "# Analyze reviewer location patterns to identify non-NYC hotels\n",
    "print(\"Analyzing reviewer geographic patterns...\")\n",
    "\n",
    "hotel_stats = []\n",
    "for hotel_name, group in exploration_df.groupby('hotel_name'):\n",
    "    location_data = group['location'].fillna('')\n",
    "\n",
    "    total_reviews = len(group)\n",
    "    total_with_location = group['location'].notna().sum()\n",
    "    uk_reviews = location_data.str.contains('United Kingdom|UK|England|Scotland|Wales', case=False).sum()\n",
    "    shanghai_reviews = location_data.str.contains('Shanghai|China', case=False).sum()\n",
    "\n",
    "    hotel_stats.append({\n",
    "        'hotel_name': hotel_name,\n",
    "        'total_reviews': total_reviews,\n",
    "        'total_with_location': total_with_location,\n",
    "        'uk_reviews': uk_reviews,\n",
    "        'shanghai_reviews': shanghai_reviews\n",
    "    })\n",
    "\n",
    "# Convert to analysis DataFrame\n",
    "hotel_analysis = pd.DataFrame(hotel_stats)\n",
    "hotel_analysis['uk_percentage'] = (hotel_analysis['uk_reviews'] / hotel_analysis['total_with_location']).fillna(0)\n",
    "hotel_analysis['shanghai_percentage'] = (hotel_analysis['shanghai_reviews'] / hotel_analysis['total_with_location']).fillna(0)\n",
    "\n",
    "# Identify problematic hotels\n",
    "uk_threshold = 0.6\n",
    "uk_hotels = hotel_analysis[\n",
    "    (hotel_analysis['uk_percentage'] > uk_threshold) &\n",
    "    (hotel_analysis['total_with_location'] >= 10)\n",
    "]\n",
    "\n",
    "print(f\"\\nHotels with >{uk_threshold*100:.0f}% UK reviewers: {len(uk_hotels)}\")\n",
    "if len(uk_hotels) > 0:\n",
    "    print(\"\\n UK-heavy hotels (likely London):\")\n",
    "    uk_display = uk_hotels.nlargest(10, 'uk_percentage')[['hotel_name', 'total_with_location', 'uk_percentage']]\n",
    "    for _, row in uk_display.iterrows():\n",
    "        print(f\"  • {row['hotel_name']} - {row['uk_percentage']:.1%} UK reviewers ({row['total_with_location']} total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e1fc02",
   "metadata": {},
   "source": [
    "#### 6B.3 Positive NYC Filtering Strategy\n",
    "\n",
    "**Approach:** Identify genuine NYC hotels using location indicators*\n",
    "\n",
    "**Strategy Shift:** Instead of removing non-NYC, actively identify NYC hotels\n",
    "\n",
    "**Indicators:** Hotel names containing NYC-specific terms\n",
    "\n",
    "**Advantage:** Reduces false positives from ambiguous neighborhood names (SoHo, Chelsea, etc.)\n",
    "\n",
    "**Final Cleanup:** Manual removal of remaining misclassified hotels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10fd92fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NYC hotels identified: 12,846 rows\n",
      "Unique NYC hotels: 127\n",
      "\n",
      " Top 10 NYC hotels:\n",
      "  • Luma Hotel Time Square (712 reviews)\n",
      "  • Hyatt Grand Central New York (595 reviews)\n",
      "  • Hyatt Centric Times Square New York (434 reviews)\n",
      "  • Hotel Riu Plaza New York Times Square (431 reviews)\n",
      "  • DoubleTree by Hilton Hotel New York Times Square West (354 reviews)\n",
      "  • Hyatt Place New York/Chelsea (334 reviews)\n",
      "  • M Social Hotel Times Square New York (291 reviews)\n",
      "  • Lotte New York Palace (284 reviews)\n",
      "  • 1 Hotel Central Park (282 reviews)\n",
      "  • Arlo Midtown (281 reviews)\n",
      "\n",
      " NYC hotels with ambiguous neighborhood terms\n",
      "\n",
      "  SoHo: 7 hotels\n",
      "    • The Soho Hotel\n",
      "    • hub by Premier Inn London Soho hotel\n",
      "    • The Z Hotel Soho\n",
      "\n",
      "  Chelsea: 9 hotels\n",
      "    • SpringHill Suites New York Manhattan/Chelsea\n",
      "    • TownePlace Suites by Marriott New York Manhattan/Chelsea\n",
      "    • Hyatt House New York/Chelsea\n"
     ]
    }
   ],
   "source": [
    "# Apply positive NYC filtering - identify genuine NYC hotels\n",
    "nyc_indicators = [\n",
    "    'New York', 'NYC', 'Manhattan', 'Brooklyn', 'Queens', 'Bronx',\n",
    "    'Times Square', 'Time Square', 'Central Park', 'Wall Street',\n",
    "    'Midtown', 'Downtown', 'Financial District', 'SoHo', 'NoMad',\n",
    "    'TriBeCa', 'Upper East', 'Upper West', 'Lower East', 'Herald Square',\n",
    "    'Penn Station', 'Grand Central', 'JFK', 'LaGuardia', 'Empire State'\n",
    "]\n",
    "\n",
    "nyc_pattern = '|'.join(nyc_indicators)\n",
    "nyc_hotels = exploration_df[exploration_df['hotel_name'].str.contains(nyc_pattern, case=False, na=False)]\n",
    "\n",
    "print(f\"NYC hotels identified: {len(nyc_hotels):,} rows\")\n",
    "print(f\"Unique NYC hotels: {nyc_hotels['hotel_name'].nunique()}\")\n",
    "\n",
    "# Check for remaining ambiguous terms that might be misclassified\n",
    "print(f\"\\n Top 10 NYC hotels:\")\n",
    "nyc_top = nyc_hotels['hotel_name'].value_counts().head(10)\n",
    "for hotel, count in nyc_top.items():\n",
    "    print(f\"  • {hotel} ({count:,} reviews)\")\n",
    "\n",
    "# Check for potentially ambiguous hotels needing manual review\n",
    "ambiguous_terms = ['SoHo', 'Chelsea', 'Greenwich', 'Victoria']\n",
    "print(f\"\\n NYC hotels with ambiguous neighborhood terms\")\n",
    "for term in ambiguous_terms:\n",
    "    matching = nyc_hotels[nyc_hotels['hotel_name'].str.contains(term, case=False, na=False)]\n",
    "    if len(matching) > 0:\n",
    "        unique_hotels = matching['hotel_name'].unique()\n",
    "        print(f\"\\n  {term}: {len(unique_hotels)} hotels\")\n",
    "        for hotel in unique_hotels[:3]:\n",
    "            print(f\"    • {hotel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a2ecce",
   "metadata": {},
   "source": [
    "#### 6B.4 Manual Cleanup of Misclassified Hotels\n",
    "*Remove remaining London hotels caught by ambiguous neighborhood names*\n",
    "\n",
    "**Issue:** \"SoHo\" exists in both NYC and London\n",
    "\n",
    "**Solution:** Remove clearly London-branded hotels\n",
    "\n",
    "**Targets:** Hotels with \"London\" in name or known London hotel chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1687d02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing London hotels:\n",
      "  • The Soho Hotel (19 reviews)\n",
      "  • The Z Hotel Soho (38 reviews)\n",
      "  • hub by Premier Inn London Soho hotel (154 reviews)\n",
      "\n",
      "Manual cleanup complete...\n",
      "Final NYC dataset: 12,635 rows\n",
      "Unique hotels: 124\n",
      "\n",
      "Remaining SoHo hotels (verified NYC):\n",
      "  • Arlo SoHo\n",
      "  • Courtyard New York Manhattan/SoHo\n",
      "  • Soho Grand Hotel\n",
      "  • Sohotel\n"
     ]
    }
   ],
   "source": [
    "# Manual removal of identified London hotels\n",
    "london_hotels_to_remove = [\n",
    "    'The Soho Hotel',                       # London SoHo hotel\n",
    "    'The Z Hotel Soho',                     # London hotel chain\n",
    "    'hub by Premier Inn London Soho hotel'  # Explicitly London-branded\n",
    "]\n",
    "\n",
    "print(f\"Removing London hotels:\")\n",
    "for hotel in london_hotels_to_remove:\n",
    "    count = nyc_hotels[nyc_hotels['hotel_name'] == hotel].shape[0]\n",
    "    print(f\"  • {hotel} ({count:,} reviews)\")\n",
    "\n",
    "# Apply manual cleanup\n",
    "final_nyc_df = nyc_hotels[~nyc_hotels['hotel_name'].isin(london_hotels_to_remove)].copy()\n",
    "\n",
    "print(f\"\\nManual cleanup complete...\")\n",
    "print(f\"Final NYC dataset: {len(final_nyc_df):,} rows\")\n",
    "print(f\"Unique hotels: {final_nyc_df['hotel_name'].nunique()}\")\n",
    "\n",
    "# Verify remaining SoHo hotels are legitimate NYC hotels\n",
    "remaining_soho = final_nyc_df[final_nyc_df['hotel_name'].str.contains('soho', case=False)]['hotel_name'].unique()\n",
    "print(f\"\\nRemaining SoHo hotels (verified NYC):\")\n",
    "for hotel in remaining_soho:\n",
    "    print(f\"  • {hotel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c964eae",
   "metadata": {},
   "source": [
    "## 7. Silver Layer: Final Geographic Filter & Save\n",
    "*Clean, validated approach - works whether exploration was run or not*\n",
    "\n",
    "**Implementation:** Apply proven NYC filter strategy\n",
    "\n",
    "**Output:** `data/silver/tripadvisor/staging/tripadvisor_nyc_2022_2025_final.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86477de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset saved\n",
      "Location: /home/anna/code/TinaKgn/tourism_data_project/notebooks/shared/data_extraction/tripadvisor/data/silver/tripadvisor/staging/tripadvisor_nyc_2022_2025_final.parquet\n",
      "Rows: 12,635\n",
      "Hotels: 124\n",
      "File size: 4.9 MB\n",
      "\n",
      "Ready for gold layer processing and analysis\n"
     ]
    }
   ],
   "source": [
    "# Set up silver staging directory\n",
    "silver_dir = project_root / \"data\" / \"silver\" / \"tripadvisor\" / \"staging\"\n",
    "silver_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_file = silver_dir / \"tripadvisor_nyc_2022_2025_final.parquet\"\n",
    "\n",
    "# Check if final filtering already completed\n",
    "if output_file.exists():\n",
    "    # Load existing file to show stats\n",
    "    existing_final = pd.read_parquet(output_file)\n",
    "    file_size_mb = output_file.stat().st_size / (1024*1024)\n",
    "    print(f\"[SKIP] Final geographic filter already complete\")\n",
    "    print(f\"Existing file: {len(existing_final):,} rows, {existing_final['hotel_name'].nunique()} hotels\")\n",
    "    print(f\"File size: {file_size_mb:.1f} MB\")\n",
    "    print(f\"Location: {output_file}\")\n",
    "    print(\"\\nTo reprocess, delete the silver/staging folder first\")\n",
    "else:\n",
    "    # Load primary filtered data (works whether exploration was run or skipped)\n",
    "    primary_filter_dir = bronze_base / \"02_primary_filter\"\n",
    "    df_for_filtering = pd.read_parquet(primary_filter_dir / \"tripadvisor_nyc_2022_2025_date_filtered.parquet\")\n",
    "\n",
    "    # Apply validated NYC filter strategy\n",
    "    nyc_indicators = [\n",
    "        'New York', 'NYC', 'Manhattan', 'Brooklyn', 'Queens', 'Bronx',\n",
    "        'Times Square', 'Time Square', 'Central Park', 'Wall Street',\n",
    "        'Midtown', 'Downtown', 'Financial District', 'SoHo', 'NoMad',\n",
    "        'TriBeCa', 'Upper East', 'Upper West', 'Lower East', 'Herald Square',\n",
    "        'Penn Station', 'Grand Central', 'JFK', 'LaGuardia', 'Empire State'\n",
    "    ]\n",
    "\n",
    "    nyc_pattern = '|'.join(nyc_indicators)\n",
    "    nyc_filtered = df_for_filtering[df_for_filtering['hotel_name'].str.contains(nyc_pattern, case=False, na=False)]\n",
    "\n",
    "    # Remove identified London hotels\n",
    "    london_hotels_to_remove = ['The Soho Hotel', 'The Z Hotel Soho', 'hub by Premier Inn London Soho hotel']\n",
    "    final_clean_df = nyc_filtered[~nyc_filtered['hotel_name'].isin(london_hotels_to_remove)].copy()\n",
    "\n",
    "    # Save to silver staging directory\n",
    "    final_clean_df.to_parquet(output_file, compression=\"snappy\")\n",
    "\n",
    "    print(f\"Final dataset saved\")\n",
    "    print(f\"Location: {output_file}\")\n",
    "    file_size_mb = output_file.stat().st_size / (1024*1024)\n",
    "    print(f\"Rows: {len(final_clean_df):,}\")\n",
    "    print(f\"Hotels: {final_clean_df['hotel_name'].nunique()}\")\n",
    "    print(f\"File size: {file_size_mb:.1f} MB\")\n",
    "    print(f\"\\nReady for gold layer processing and analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3ee782",
   "metadata": {},
   "source": [
    "## 8. Final Verification & Cleanup\n",
    "\n",
    "*Verify saved dataset and optional cleanup of intermediate files*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39a1bcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Dataset Verification:\n",
      "==================================================\n",
      "File loads successfully\n",
      "Shape: (12635, 15)\n",
      "Unique hotels: 124\n",
      "\n",
      "Column info:\n",
      "   • Total columns: 15\n",
      "   • Date range sample: ['Jan 2023', 'Aug 2022', 'Jan 2023']\n",
      "   • Top 3 hotels:\n",
      "     - Luma Hotel Time Square: 712 reviews\n",
      "     - Hyatt Grand Central New York: 595 reviews\n",
      "     - Hyatt Centric Times Square New York: 434 reviews\n",
      "\n",
      "Known Column Issues (to address in gold layer):\n",
      "   • Dummy columns found: ['col_0', 'Unnamed: 0']\n",
      "   • These are Excel conversion artifacts to be cleaned in gold processing\n",
      "\n",
      "File Storage Summary:\n",
      "   • Raw chunks (bronze/01_raw_conversion): ~169 MB\n",
      "   • Primary filter (bronze/02_primary_filter): 17.7 MB\n",
      "   • Final dataset (silver/tripadvisor): 4.9 MB\n",
      "\n",
      "Optional Cleanup:\n",
      "   • To save disk space, you can delete intermediate processing files:\n",
      "   • rm -rf /home/anna/code/TinaKgn/tourism_data_project/notebooks/shared/data_extraction/tripadvisor/data/bronze/tripadvisor/01_raw_conversion\n",
      "   • rm -rf /home/anna/code/TinaKgn/tourism_data_project/notebooks/shared/data_extraction/tripadvisor/data/bronze/tripadvisor/02_primary_filter\n",
      "   • Keeps: original Excel + final silver parquet (161.8 MB total)\n",
      "\n",
      "Workflow complete! Ready for gold layer processing.\n"
     ]
    }
   ],
   "source": [
    "# Load and verify final saved dataset\n",
    "print(\"Final Dataset Verification:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "final_saved = pd.read_parquet(output_file)\n",
    "print(f\"File loads successfully\")\n",
    "print(f\"Shape: {final_saved.shape}\")\n",
    "print(f\"Unique hotels: {final_saved['hotel_name'].nunique()}\")\n",
    "\n",
    "# Quick data quality check\n",
    "print(f\"\\nColumn info:\")\n",
    "print(f\"   • Total columns: {len(final_saved.columns)}\")\n",
    "print(f\"   • Date range sample: {final_saved['date'].dropna().head(3).tolist()}\")\n",
    "print(f\"   • Top 3 hotels:\")\n",
    "for hotel, count in final_saved['hotel_name'].value_counts().head(3).items():\n",
    "    print(f\"     - {hotel}: {count:,} reviews\")\n",
    "\n",
    "# Document known column issues for gold layer processing\n",
    "print(f\"\\nKnown Column Issues (to address in gold layer):\")\n",
    "dummy_columns = [col for col in final_saved.columns if 'Unnamed:' in str(col) or col in ['col_0']]\n",
    "if dummy_columns:\n",
    "    print(f\"   • Dummy columns found: {dummy_columns}\")\n",
    "    print(f\"   • These are Excel conversion artifacts to be cleaned in gold processing\")\n",
    "else:\n",
    "    print(f\"   • No dummy columns detected\")\n",
    "\n",
    "print(f\"\\nFile Storage Summary:\")\n",
    "conversion_size = sum(f.stat().st_size for f in conversion_dir.glob(\"*.parquet\")) / (1024*1024)\n",
    "primary_filter_size = (bronze_base / \"02_primary_filter\" / \"tripadvisor_nyc_2022_2025_date_filtered.parquet\").stat().st_size / (1024*1024)\n",
    "print(f\"   • Raw chunks (bronze/01_raw_conversion): ~{conversion_size:.0f} MB\")\n",
    "print(f\"   • Primary filter (bronze/02_primary_filter): {primary_filter_size:.1f} MB\")\n",
    "print(f\"   • Final dataset (silver/tripadvisor): {file_size_mb:.1f} MB\")\n",
    "\n",
    "print(f\"\\nOptional Cleanup:\")\n",
    "print(f\"   • To save disk space, you can delete intermediate processing files:\")\n",
    "print(f\"   • rm -rf {conversion_dir}\")\n",
    "print(f\"   • rm -rf {bronze_base}/02_primary_filter\")\n",
    "print(f\"   • Keeps: original Excel + final silver parquet ({156.9 + file_size_mb:.1f} MB total)\")\n",
    "print(f\"\\nWorkflow complete! Ready for gold layer processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c64361",
   "metadata": {},
   "source": [
    "## 9. Next Steps: Gold Layer Processing\n",
    "\n",
    "**Current Status:**\n",
    "Bronze → Silver workflow complete for TripAdvisor NYC dataset\n",
    "\n",
    "**Upcoming Gold Layer Integration:**\n",
    "\n",
    "- **Multi-dataset analysis:** All processed silver datasets (TripAdvisor NYC, Yelp New Orleans, AirBnB LA/Chicago) will be explored for shared columns\n",
    "  \n",
    "- **Schema standardization:** Common fields (location, date, rating, text) will be unified across datasets\n",
    "  \n",
    "- **Data quality:** Null value handling, strategic imputation, and appropriate data type conversions\n",
    "  \n",
    "- **Analysis-ready format:** Final gold datasets optimized for sentiment analysis and tourism correlation modeling\n",
    "\n",
    "**Gold Processing Pipeline:**\n",
    "1. Load all silver datasets and analyze column overlap\n",
    "2. Standardize shared column names and formats\n",
    "3. Handle missing values with dataset-appropriate strategies\n",
    "4. Convert data types for analysis efficiency\n",
    "5. Create unified gold datasets for cross-platform analysis\n",
    "\n",
    "<!-- **Template Replication:** Use this notebook structure for remaining datasets:\n",
    "- `002_yelp_new_orleans_2013_2016_2018_extraction.ipynb`\n",
    "- `003_airbnb_los_angeles_2022_2024_extraction.ipynb`\n",
    "- `004_airbnb_chicago_2022_2024_extraction.ipynb` -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12f05f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs cleared for clean collaboration\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "Jupyter.notebook.get_cells().forEach(function(c) {c.set_input_prompt();})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear all cell outputs for clean notebook sharing\n",
    "\n",
    "from IPython.display import Javascript\n",
    "Javascript(\"Jupyter.notebook.clear_all_output()\")\n",
    "print(\"Outputs cleared for clean collaboration\")\n",
    "\n",
    "# Reset all cell numbers to None\n",
    "Javascript(\"Jupyter.notebook.get_cells().forEach(function(c) {c.set_input_prompt();})\")\n",
    "\n",
    "# # Verify no local paths\n",
    "# import json\n",
    "# with open('001_tripadvisor_nyc_extraction.ipynb') as f:\n",
    "#     nb = json.load(f)\n",
    "#     nb_str = json.dumps(nb)\n",
    "#     if '[!Replace with Local user directory root!]' in nb_str:\n",
    "#         print(\"Local paths found in notebook metadata\")\n",
    "#     else:\n",
    "#         print(\"No local paths detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793bb852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
