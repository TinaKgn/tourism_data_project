{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a70314f",
   "metadata": {},
   "source": [
    "# Tourism Sentiment Analysis - AirBnB Data Extraction\n",
    "\n",
    "**Project:** Tourism Sentiment Analysis\n",
    "\n",
    "**Task:** Data Extraction & Processing\n",
    "\n",
    "**Dataset Source:** InsideAirbnb\n",
    "\n",
    "**Focus:** Chicago/Los Angeles, 2022-2024, Post-COVID Analysis\n",
    "\n",
    "**Source:** http://insideairbnb.com/get-the-data/\n",
    "\n",
    "---\n",
    "<br>\n",
    "<details>\n",
    "<summary><strong>Dataset Update Warning</strong> (click to expand)</summary>\n",
    "\n",
    "\n",
    "\n",
    "InsideAirbnb regularly updates their datasets with new timestamps.\n",
    "\n",
    "Download URLs contain dates that change over time (e.g., `2025-06-17` → `2025-12-XX`).\n",
    "\n",
    "Historical data may be removed or restructured.\n",
    "\n",
    "This notebook includes validation to detect significant dataset changes.\n",
    "</details>\n",
    "\n",
    "---\n",
    "<br>\n",
    "<details>\n",
    "<summary><strong>Post-COVID Focus</strong> (click to expand)</summary>\n",
    "\n",
    "This analysis focuses on post-pandemic tourism patterns (2022+) to understand how tourism volume fluctuations affect customer sentiment in hospitality reviews.\n",
    "\n",
    "The workflow filters out pre-2022 data as the first major data reduction step.\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fff1ae",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration\n",
    "*Import libraries, set up project paths, create directory structure*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb74de6d",
   "metadata": {},
   "source": [
    "### 1A. Imports & Script Setup\n",
    "*Load required packages and configure script imports*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911f5b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# File handling & web requests\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# System utilities\n",
    "import sys\n",
    "\n",
    "# Bootstrap: Add shared scripts to Python path\n",
    "def setup_scripts_path():\n",
    "    \"\"\"Add shared scripts to Python path\"\"\"\n",
    "    current = Path.cwd()\n",
    "    for _ in range(10):\n",
    "        if (current / '.projectroot').exists():\n",
    "            scripts_dir = current / 'notebooks' / 'shared' / 'scripts'\n",
    "            if scripts_dir.exists():\n",
    "                sys.path.insert(0, str(scripts_dir))\n",
    "                return scripts_dir\n",
    "        if current.parent == current:\n",
    "            break\n",
    "        current = current.parent\n",
    "    raise FileNotFoundError(\n",
    "        \"scripts directory not found.\\n\"\n",
    "        \"ensure .projectroot exists at project root.\"\n",
    "    )\n",
    "\n",
    "# Setup path and import utilities\n",
    "scripts_dir = setup_scripts_path()\n",
    "\n",
    "from project_utils import find_project_root\n",
    "from data_io import setup_extraction_directories, check_existing_file, check_existing_chunks\n",
    "from data_validation import print_final_summary, print_storage_summary\n",
    "\n",
    "print(f\"✓ Packages and scripts loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06c4aaa",
   "metadata": {},
   "source": [
    "### 1B. Project Root Detection\n",
    "*Cross-platform function to locate project directory automatically*\n",
    "\n",
    "**Purpose:** Finds project root by searching for `.projectroot` marker file\n",
    "\n",
    "**Handles:** Working directory issues, different operating systems, various notebook locations\n",
    "\n",
    "**Confirmation:** Displays detected path for verification\n",
    "\n",
    "**Manual Override:** Uncomment line below if auto-detection fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa15d37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically detect project root\n",
    "project_root = find_project_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44a66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If auto-detection fails, uncomment and edit this line:\n",
    "# project_root = Path('/.../.../.../[tourism_data_project]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4045c5",
   "metadata": {},
   "source": [
    "### 1C. Set Project Paths\n",
    "*Establish standardized directory structure for bronze and silver processing*\n",
    "\n",
    "**Bronze Structure:** Raw download → chunked conversion → primary filter\n",
    "\n",
    "**Silver Structure:** Final staging area for gold layer integration\n",
    "\n",
    "**Auto-creation:** All directories created automatically for new collaborators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5defa572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base AirBnB directory structure\n",
    "# City-specific subdirectories created after selection in Section 2\n",
    "bronze_base = project_root / \"data\" / \"bronze\" / \"airbnb\"\n",
    "silver_base = project_root / \"data\" / \"silver\" / \"airbnb\"\n",
    "\n",
    "bronze_base.mkdir(parents=True, exist_ok=True)\n",
    "silver_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Bronze base: {bronze_base}\")\n",
    "print(f\"Silver base: {silver_base}\")\n",
    "print(\"Directory structure created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839b1a25",
   "metadata": {},
   "source": [
    "## 2. Data Acquisition\n",
    "*Download raw CSV files from InsideAirbnb for Chicago or Los Angeles*\n",
    "\n",
    "**City Selection:** Configure `CITY` variable below (chicago or los_angeles)\n",
    "\n",
    "**Automatic Detection:** Tests recent dates to find current data availability\n",
    "\n",
    "<details>\n",
    "<summary><strong>Manual Download Instructions</strong> (click to expand)</summary>\n",
    "\n",
    "***If automated download fails:***\n",
    "\n",
    "1. Visit: https://insideairbnb.com/get-the-data/\n",
    "2. Navigate to Chicago or Los Angeles section\n",
    "3. Download current: `listings.csv.gz` and `reviews.csv.gz`\n",
    "4. Rename to: `{city}_listings.csv.gz`, `{city}_reviews.csv.gz`\n",
    "5. Place in: `data/bronze/airbnb/{city}/00_original_download/`\n",
    "\n",
    "**Note:** InsideAirbnb updates datasets regularly. If automated download fails, manually download current files and adjust date references if needed.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56c8c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import AirBnB-specific utilities\n",
    "from airbnb_utils import download_insideairbnb\n",
    "\n",
    "# USER CONFIGURATION - City Selection\n",
    "CITY = \"chicago\"            # Options: \"chicago\" or \"los_angeles\"\n",
    "\n",
    "print(f\"Current city: {CITY.replace('_', ' ').title()}\")\n",
    "print(\"To change city: Update |CITY| variable above and rerun this cell\")\n",
    "\n",
    "# Create city-specific directories\n",
    "city_bronze_base = bronze_base / CITY\n",
    "city_silver_base = silver_base / CITY\n",
    "city_bronze_base.mkdir(parents=True, exist_ok=True)\n",
    "city_silver_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nBronze directory: {city_bronze_base}\")\n",
    "print(f\"Silver directory: {city_silver_base}\")\n",
    "print(\"\\nReady for download configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e71022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download configuration\n",
    "DATE_SNAPSHOT = \"2025-06-17\"  # Update if needed based on InsideAirbnb availability\n",
    "\n",
    "original_dir = city_bronze_base / \"00_original_download\"\n",
    "original_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Testing download for {CITY.replace('_', ' ').title()} ({DATE_SNAPSHOT})...\")\n",
    "\n",
    "# Download listings\n",
    "listings_path = original_dir / f\"{CITY}_listings.csv.gz\"\n",
    "success, message = download_insideairbnb(\n",
    "    city=CITY,\n",
    "    date_snapshot=DATE_SNAPSHOT,\n",
    "    file_type='listings',\n",
    "    output_path=listings_path\n",
    ")\n",
    "print(f\"Listings: {message}\")\n",
    "\n",
    "# Download reviews\n",
    "reviews_path = original_dir / f\"{CITY}_reviews.csv.gz\"\n",
    "success, message = download_insideairbnb(\n",
    "    city=CITY,\n",
    "    date_snapshot=DATE_SNAPSHOT,\n",
    "    file_type='reviews',\n",
    "    output_path=reviews_path\n",
    ")\n",
    "print(f\"Reviews: {message}\")\n",
    "\n",
    "print(f\"\\nFiles location: {original_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79c3b38",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<u>***City Configuration Checkpoint***</u>\n",
    "\n",
    "**Project Scope:** Chicago & Los Angeles (2022, 2024) comparative analysis  \n",
    "\n",
    "**Reset Point:** Update `CITY` variable above and rerun from here for different city\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4389758",
   "metadata": {},
   "source": [
    "## 3. Bronze Layer: Raw Data Processing\n",
    "*Load CSV files, merge datasets, convert to chunked parquet preserving original structure*\n",
    "\n",
    "**Input:** `data/bronze/airbnb/{city}/00_original_download/{city}_listings.csv.gz` & `{city}_reviews.csv.gz`\n",
    "\n",
    "**Output:** `data/bronze/airbnb/{city}/01_raw_conversion/{city}_merged_chunk_*.parquet`\n",
    "\n",
    "**Processing:** Merge listings + reviews, then chunked conversion for memory efficiency\n",
    "\n",
    "**Purpose:** Preserve complete dataset structure while converting to analysis-friendly format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeedc251",
   "metadata": {},
   "source": [
    "### 3A.1 Load and Verify Raw Files\n",
    "*Load downloaded CSV files and verify structure before processing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee30489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import AirBnB validation\n",
    "from airbnb_utils import validate_insideairbnb_structure\n",
    "\n",
    "# Load and verify downloaded files\n",
    "listings_file = original_dir / f\"{CITY}_listings.csv.gz\"\n",
    "reviews_file = original_dir / f\"{CITY}_reviews.csv.gz\"\n",
    "\n",
    "# Verify files exist\n",
    "if not listings_file.exists():\n",
    "    print(f\"Listings file not found: {listings_file}\")\n",
    "    print(\"Please run Section 2 to download files first\")\n",
    "elif not reviews_file.exists():\n",
    "    print(f\"Reviews file not found: {reviews_file}\")\n",
    "    print(\"Please run Section 2 to download files first\")\n",
    "else:\n",
    "    print(\"✓ Both files found, loading data...\")\n",
    "\n",
    "    # Load files\n",
    "    print(\"\\nLoading listings...\")\n",
    "    listings_df = pd.read_csv(listings_file, compression=\"gzip\")\n",
    "    print(f\"Listings shape: {listings_df.shape}\")\n",
    "\n",
    "    print(\"\\nLoading reviews...\")\n",
    "    reviews_df = pd.read_csv(reviews_file, compression=\"gzip\")\n",
    "    print(f\"Reviews shape: {reviews_df.shape}\")\n",
    "\n",
    "    # Validate structure\n",
    "    valid, missing = validate_insideairbnb_structure(reviews_df, listings_df)\n",
    "\n",
    "    if not valid:\n",
    "        print(\"\\n✗ Dataset structure validation failed:\")\n",
    "        if missing['reviews']:\n",
    "            print(f\"  Missing review columns: {missing['reviews']}\")\n",
    "        if missing['listings']:\n",
    "            print(f\"  Missing listing columns: {missing['listings']}\")\n",
    "    else:\n",
    "        print(\"\\nDataset structure validated\")\n",
    "        print(f\"  Unique listings: {listings_df['id'].nunique():,}\")\n",
    "        print(f\"  Unique listing IDs in reviews: {reviews_df['listing_id'].nunique():,}\")\n",
    "        print(\"\\nRaw data loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20a7982",
   "metadata": {},
   "source": [
    "#### 3A.2 Raw Data Structure Validation  \n",
    "*Verify downloaded file structure before merging*\n",
    "\n",
    "**Purpose:** Detect InsideAirbnb format changes and validate required columns\n",
    "\n",
    "**Expected:** Reviews (6 cols), Listings (~79 cols) based on Nov 2025 Chicago dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e184ebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data structure validation using future-proof checks\n",
    "print(\"Raw Data Structure Validation:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Use validation function\n",
    "valid, missing = validate_insideairbnb_structure(reviews_df, listings_df)\n",
    "\n",
    "# Report structure\n",
    "print(f\"Structure check:\")\n",
    "print(f\"  Reviews: {len(reviews_df.columns)} columns\")\n",
    "print(f\"  Listings: {len(listings_df.columns)} columns\")\n",
    "\n",
    "if not valid:\n",
    "    print(f\"\\nMissing required columns:\")\n",
    "    if missing['reviews']:\n",
    "        print(f\"  Reviews: {missing['reviews']}\")\n",
    "    if missing['listings']:\n",
    "        print(f\"  Listings: {missing['listings']}\")\n",
    "    print(f\"\\nDataset may be incompatible - check InsideAirbnb version\")\n",
    "else:\n",
    "    print(f\"\\n✓ All required columns present\")\n",
    "    print(f\"✓ Ready for merge operation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450efacd",
   "metadata": {},
   "source": [
    "### 3B. Merge Listings and Reviews\n",
    "*Combine datasets following Anna's proven methodology*\n",
    "\n",
    "**Pattern:** Left join `reviews` to `listings` to preserve all review data with listing details\n",
    "\n",
    "**Join Strategy:** `reviews.listing_id` → `listings.id`\n",
    "\n",
    "**Result:** Comprehensive dataset with review content + listing characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f6f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import merge function\n",
    "from airbnb_utils import merge_listings_reviews\n",
    "\n",
    "# Merge listings and reviews\n",
    "print(\"Merging reviews with listings...\")\n",
    "print(\"Join strategy: reviews (left) + listings (right) on listing_id = id\")\n",
    "\n",
    "merged_data = merge_listings_reviews(\n",
    "    listings_path=listings_file,\n",
    "    reviews_path=reviews_file\n",
    ")\n",
    "\n",
    "print(f\"\\nMerge complete:\")\n",
    "print(f\"Merged records: {len(merged_data):,}\")\n",
    "print(f\"Merged dataset shape: {merged_data.shape}\")\n",
    "\n",
    "# Verify merge quality - check for unmatched reviews\n",
    "missing_listings = merged_data['id_listing'].isna().sum()\n",
    "if missing_listings > 0:\n",
    "    print(f\"\\n✗ {missing_listings:,} reviews could not be matched to listings\")\n",
    "else:\n",
    "    print(\"\\n✓ All reviews successfully matched to listings\")\n",
    "\n",
    "# Show sample\n",
    "key_cols = ['listing_id', 'date', 'comments', 'name', 'property_type']\n",
    "available_cols = [col for col in key_cols if col in merged_data.columns]\n",
    "print(f\"\\nSample merged data:\")\n",
    "print(merged_data[available_cols].head(3))\n",
    "\n",
    "print(\"\\nReady for year filtering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c915ca",
   "metadata": {},
   "source": [
    "### 3C. Primary Filters: Year, Business Customer & Data Quality\n",
    "*Apply core filters immediately to reduce dataset before saving*\n",
    "\n",
    "**Input:** Merged dataset (all available reviews, all available years, all property types)\n",
    "\n",
    "**Filters Applied:**\n",
    "\n",
    "1. **Year filter:** Reviews from 2022+ only *(post-COVID focus)*\n",
    "   \n",
    "2. **Business customer filter:** Entire home/apartment rentals only (exclude private/shared rooms)\n",
    "\n",
    "**Output:** `data/bronze/airbnb/{city}/02_primary_filter/{city}_post2022_filtered.parquet`\n",
    "\n",
    "**Purpose:** Focus on business travel market with current, relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c98e6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up primary filter output directory\n",
    "primary_filter_dir = city_bronze_base / \"02_primary_filter\"\n",
    "primary_filter_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_file = primary_filter_dir / f\"{CITY}_post2022_filtered.parquet\"\n",
    "\n",
    "# Check if filtering already completed\n",
    "exists, info = check_existing_file(output_file, file_type='parquet')\n",
    "\n",
    "if not exists:\n",
    "    print(\"Applying primary filters...\")\n",
    "    print(f\"\\nOriginal merged data: {len(merged_data):,} rows, {len(merged_data.columns)} columns\")\n",
    "\n",
    "    # Filter 1: Year filter (2022+)\n",
    "    merged_data['date'] = pd.to_datetime(merged_data['date'], errors='coerce')\n",
    "    year_filtered = merged_data[merged_data['date'].dt.year >= 2022].copy()\n",
    "    print(f\"  After year filter (2022+): {len(year_filtered):,} rows\")\n",
    "\n",
    "    # Filter 2: Business customer filter (Entire home/apt + Hotel room)\n",
    "    business_filtered = year_filtered[\n",
    "        year_filtered['room_type'].isin(['Entire home/apt', 'Hotel room'])\n",
    "    ].copy()\n",
    "    print(f\"  After business customer filter: {len(business_filtered):,} rows\")\n",
    "\n",
    "    # Filter 3: Column optimization - keep only sentiment-relevant columns\n",
    "    keep_columns = [\n",
    "        # Review essentials\n",
    "        'listing_id', 'id_review', 'date', 'reviewer_id', 'comments',\n",
    "\n",
    "        # Property context\n",
    "        'name', 'description', 'neighborhood_overview',\n",
    "        'room_type', 'accommodates', 'bedrooms', 'beds', 'bathrooms', 'price',\n",
    "        'amenities',\n",
    "\n",
    "        # Location\n",
    "        'latitude', 'longitude',\n",
    "\n",
    "        # Host quality\n",
    "        'host_id', 'host_since', 'host_about',\n",
    "        'host_response_time', 'host_response_rate', 'host_is_superhost',\n",
    "        'host_listings_count',\n",
    "\n",
    "        # Review scores\n",
    "        'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness',\n",
    "        'review_scores_checkin', 'review_scores_communication', 'review_scores_location',\n",
    "        'review_scores_value',\n",
    "\n",
    "        # Metadata\n",
    "        'first_review', 'last_review', 'license', 'listing_url'\n",
    "    ]\n",
    "\n",
    "    # Filter to available columns only\n",
    "    available_keep = [col for col in keep_columns if col in business_filtered.columns]\n",
    "    final_filtered = business_filtered[available_keep].copy()\n",
    "\n",
    "    print(f\"  After column optimization: {len(final_filtered.columns)} columns kept\")\n",
    "\n",
    "    # Save filtered data\n",
    "    final_filtered.to_parquet(output_file, compression=\"snappy\")\n",
    "\n",
    "    print(f\"\\nPrimary filtering complete:\")\n",
    "    print(f\"  Rows kept: {len(final_filtered):,}\")\n",
    "    print(f\"  Rows removed: {len(merged_data) - len(final_filtered):,}\")\n",
    "    print(f\"  Retention rate: {len(final_filtered)/len(merged_data)*100:.1f}%\")\n",
    "    print(f\"  Columns kept: {len(final_filtered.columns)} of {len(merged_data.columns)}\")\n",
    "    print(f\"\\nSaved to: {output_file}\")\n",
    "\n",
    "    file_size_mb = output_file.stat().st_size / (1024*1024)\n",
    "    print(f\"File size: {file_size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722c5f77",
   "metadata": {},
   "source": [
    "## 4. Load and Verify Filtered Data\n",
    "*Load primary filtered data and confirm filtering results*\n",
    "\n",
    "**Input:** `data/bronze/airbnb/{city}/02_primary_filter/{city}_post2022_filtered.parquet`\n",
    "\n",
    "**Purpose:** Verify primary filtering results and analyze data distribution\n",
    "\n",
    "**Next:** Year selection for targeted analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886564d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load primary filtered data\n",
    "primary_filter_file = primary_filter_dir / f\"{CITY}_post2022_filtered.parquet\"\n",
    "\n",
    "if not primary_filter_file.exists():\n",
    "    print(\"Primary filter file not found - run Section 3 first\")\n",
    "    print(f\"Expected location: {primary_filter_file}\")\n",
    "else:\n",
    "    exploration_df = pd.read_parquet(primary_filter_file)\n",
    "\n",
    "    print(\"✓ Primary filtered data loaded successfully\")\n",
    "    print(f\"Rows: {len(exploration_df):,}\")\n",
    "    print(f\"Columns: {len(exploration_df.columns)}\")\n",
    "    print(f\"File: {primary_filter_file.name}\")\n",
    "\n",
    "    # Verify filters applied correctly\n",
    "    print(f\"\\nFilter verification:\")\n",
    "\n",
    "    # Year distribution\n",
    "    exploration_df['date'] = pd.to_datetime(exploration_df['date'], errors='coerce')\n",
    "    year_dist = exploration_df['date'].dt.year.value_counts().sort_index()\n",
    "    print(f\"  Year distribution:\")\n",
    "    for year, count in year_dist.items():\n",
    "        print(f\"    {year}: {count:,} reviews\")\n",
    "\n",
    "    # Room type distribution\n",
    "    room_dist = exploration_df['room_type'].value_counts()\n",
    "    print(f\"\\n  Room type distribution:\")\n",
    "    for room_type, count in room_dist.items():\n",
    "        print(f\"    {room_type}: {count:,} reviews\")\n",
    "\n",
    "    print(f\"\\n✓ Filters confirmed - ready for year selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c89ad91",
   "metadata": {},
   "source": [
    "## 5. Target Year Selection\n",
    "*Configure specific year for detailed seasonal analysis*\n",
    "\n",
    "**Available Years:** 2022-2025 with strong representation\n",
    "\n",
    "**Default:** 2024 (most complete recent year)\n",
    "\n",
    "**Strategy:** Process one year at a time for focused analysis\n",
    "\n",
    "**Next:** Exploratory validation of selected year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b31561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER CONFIGURATION - Target Year Selection\n",
    "TARGET_YEAR = 2024          # Current options: 2022, 2023, 2024, 2025\n",
    "\n",
    "print(f\"Current target year: {TARGET_YEAR}\")\n",
    "print(\"To change year: Update |TARGET_YEAR| variable above and rerun this cell\")\n",
    "\n",
    "# Validate target year availability\n",
    "available_years = exploration_df['date'].dt.year.unique()\n",
    "available_years = sorted([y for y in available_years if pd.notna(y)])\n",
    "\n",
    "if TARGET_YEAR in available_years:\n",
    "    # Filter to target year\n",
    "    year_mask = exploration_df['date'].dt.year == TARGET_YEAR\n",
    "    year_df = exploration_df[year_mask].copy()\n",
    "\n",
    "    print(f\"\\nTarget year {TARGET_YEAR} is available\")\n",
    "    print(f\"Records: {len(year_df):,}\")\n",
    "\n",
    "    # Show year context\n",
    "    print(f\"\\nYear context:\")\n",
    "    for year in available_years:\n",
    "        count = (exploration_df['date'].dt.year == year).sum()\n",
    "        marker = \" <- TARGET\" if year == TARGET_YEAR else \"\"\n",
    "        print(f\"  {year}: {count:,} reviews{marker}\")\n",
    "\n",
    "    print(f\"\\nConfiguration summary:\")\n",
    "    print(f\"  City: {CITY.title()}\")\n",
    "    print(f\"  Target year: {TARGET_YEAR}\")\n",
    "    print(f\"  Records: {len(year_df):,}\")\n",
    "    print(\"\\nReady for exploratory validation\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n✗ Target year {TARGET_YEAR} not available\")\n",
    "    print(f\"Available years: {available_years}\")\n",
    "    print(\"Update TARGET_YEAR to an available year above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a95df07",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<u>***Year Configuration Checkpoint***</u>\n",
    "\n",
    "**Project Scope:** Chicago & Los Angeles (2022, 2024) comparative analysis\n",
    "\n",
    "**Reset Point:** Update `TARGET_YEAR` variable above and rerun from here for different year\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94643f74",
   "metadata": {},
   "source": [
    "## 6. Exploratory Analysis\n",
    "*Validate data richness and distribution before final save*\n",
    "\n",
    "**Purpose:** Ensure selected year has sufficient data quality for sentiment analysis\n",
    "\n",
    "**Strategy:** Examine seasonal patterns, property distribution, review completeness\n",
    "\n",
    "**Note:** *Optional section - can skip to Section 7 for direct save*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f52cd4",
   "metadata": {},
   "source": [
    "### 6A. Seasonal Distribution\n",
    "*Examine temporal patterns for tourism insights*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0569e330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal distribution analysis\n",
    "print(f\"Seasonal Analysis - {CITY.title()} {TARGET_YEAR}:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract month information\n",
    "year_df['month'] = year_df['date'].dt.month\n",
    "year_df['month_name'] = year_df['date'].dt.month_name()\n",
    "\n",
    "# Monthly distribution\n",
    "monthly_counts = year_df['month'].value_counts().sort_index()\n",
    "print(f\"Monthly Review Distribution:\")\n",
    "for month in range(1, 13):\n",
    "    if month in monthly_counts.index:\n",
    "        count = monthly_counts[month]\n",
    "        month_name = year_df[year_df['month'] == month]['month_name'].iloc[0]\n",
    "        percentage = count / len(year_df) * 100\n",
    "        print(f\"  {month_name}: {count:,} reviews ({percentage:.1f}%)\")\n",
    "\n",
    "# Seasonal groupings\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Fall'\n",
    "\n",
    "year_df['season'] = year_df['month'].apply(get_season)\n",
    "seasonal_counts = year_df['season'].value_counts()\n",
    "\n",
    "print(f\"\\nSeasonal Distribution:\")\n",
    "for season in ['Winter', 'Spring', 'Summer', 'Fall']:\n",
    "    if season in seasonal_counts.index:\n",
    "        count = seasonal_counts[season]\n",
    "        percentage = count / len(year_df) * 100\n",
    "        print(f\"  {season}: {count:,} reviews ({percentage:.1f}%)\")\n",
    "\n",
    "# Data sufficiency check\n",
    "min_seasonal = seasonal_counts.min()\n",
    "min_seasonal_pct = (min_seasonal / len(year_df)) * 100\n",
    "\n",
    "print(f\"\\nData Sufficiency:\")\n",
    "print(f\"  Minimum seasonal representation: {min_seasonal:,} reviews ({min_seasonal_pct:.1f}%)\")\n",
    "if min_seasonal_pct >= 10.0:\n",
    "    print(\"  Sufficient data for all seasons (>10% minimum)\")\n",
    "else:\n",
    "    print(\"  ✗ Limited data in some seasons (<10% minimum)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fad1064",
   "metadata": {},
   "source": [
    "### 6B. Property & Review Quality\n",
    "*Validate property diversity and review completeness*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfc0a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Property and review quality analysis\n",
    "print(f\"Property & Review Quality - {CITY.title()} {TARGET_YEAR}:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Property diversity\n",
    "print(f\"Property Coverage:\")\n",
    "print(f\"  Unique listings: {year_df['listing_id'].nunique():,}\")\n",
    "print(f\"  Unique reviewers: {year_df['reviewer_id'].nunique():,}\")\n",
    "\n",
    "# Room type breakdown\n",
    "print(f\"\\nRoom Type Distribution:\")\n",
    "room_type_dist = year_df['room_type'].value_counts()\n",
    "for room_type, count in room_type_dist.items():\n",
    "    percentage = count / len(year_df) * 100\n",
    "    print(f\"  {room_type}: {count:,} reviews ({percentage:.1f}%)\")\n",
    "\n",
    "# Review completeness\n",
    "print(f\"\\nReview Completeness:\")\n",
    "comments_complete = (~year_df['comments'].isna()).sum()\n",
    "comments_pct = comments_complete / len(year_df) * 100\n",
    "print(f\"  Comments available: {comments_complete:,} ({comments_pct:.1f}%)\")\n",
    "\n",
    "# Review score availability\n",
    "if 'review_scores_rating' in year_df.columns:\n",
    "    scores_available = year_df['review_scores_rating'].notna().sum()\n",
    "    scores_pct = scores_available / len(year_df) * 100\n",
    "    print(f\"  Review scores available: {scores_available:,} ({scores_pct:.1f}%)\")\n",
    "\n",
    "# Host diversity\n",
    "if 'host_id' in year_df.columns:\n",
    "    print(f\"\\nHost Diversity:\")\n",
    "    unique_hosts = year_df['host_id'].nunique()\n",
    "    print(f\"  Unique hosts: {unique_hosts:,}\")\n",
    "    avg_reviews_per_host = len(year_df) / unique_hosts\n",
    "    print(f\"  Average reviews per host: {avg_reviews_per_host:.1f}\")\n",
    "\n",
    "print(f\"\\nIf data quality suffices, ready for silver layer save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897fde68",
   "metadata": {},
   "source": [
    "## 7. Silver Layer: Final Save\n",
    "*Save validated year-specific dataset to silver staging*\n",
    "\n",
    "**Input:** Validated year data from exploratory analysis\n",
    "\n",
    "**Output:** `data/silver/airbnb/{city}/staging/{city}_{target_year}_final.parquet`\n",
    "\n",
    "**Purpose:** Create year-specific dataset ready for silver staging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2746b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up silver staging directory\n",
    "city_silver_base = project_root / \"data\" / \"silver\" / \"airbnb\" / CITY / \"staging\"\n",
    "city_silver_base.mkdir(parents=True, exist_ok=True)\n",
    "output_file = city_silver_base / f\"{CITY}_{TARGET_YEAR}_final.parquet\"\n",
    "\n",
    "# Check if save already completed\n",
    "exists, info = check_existing_file(output_file, file_type='parquet')\n",
    "\n",
    "if not exists:\n",
    "    # Remove exploration columns (month, month_name, season)\n",
    "    exploration_cols = ['month', 'month_name', 'season']\n",
    "    clean_df = year_df.drop(columns=[col for col in exploration_cols if col in year_df.columns])\n",
    "\n",
    "    # Save clean year-specific data to silver\n",
    "    clean_df.to_parquet(output_file, compression=\"snappy\")\n",
    "\n",
    "    file_size_mb = output_file.stat().st_size / (1024*1024)\n",
    "    print(f\"Silver layer save complete:\")\n",
    "    print(f\"  Dataset: {CITY.title()} {TARGET_YEAR}\")\n",
    "    print(f\"  Records: {len(clean_df):,}\")\n",
    "    print(f\"  Columns: {len(clean_df.columns)}\")\n",
    "    print(f\"  File size: {file_size_mb:.1f} MB\")\n",
    "    print(f\"\\nSaved to: {output_file}\")\n",
    "    print(f\"\\n✓ Ready for silver stage processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682ee3f1",
   "metadata": {},
   "source": [
    "## 8. Final Verification & Summary\n",
    "*Verify saved dataset and review workflow completion*\n",
    "\n",
    "**Purpose:** Confirm data integrity and provide workflow summary\n",
    "\n",
    "**Next:** Gold layer integration or repeat workflow for different city/year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e48f828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comprehensive summary\n",
    "print_final_summary(\n",
    "    output_file,\n",
    "    dataset_name=f\"AirBnB {CITY.replace('_', ' ').title()} {TARGET_YEAR}\",\n",
    "    file_type='parquet'\n",
    ")\n",
    "\n",
    "# Workflow completion status\n",
    "print(f\"\\nWorkflow Status:\")\n",
    "print(f\"  City: {CITY.replace('_', ' ').title()} ✓\")\n",
    "print(f\"  Year: {TARGET_YEAR} ✓\")\n",
    "print(f\"  Records: {len(year_df):,} ✓\")\n",
    "print(f\"  File: {output_file.name} ✓\")\n",
    "\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  - Change |CITY| variable (Section 2) for different city\")\n",
    "print(f\"  - Change |TARGET_YEAR| variable (Section 5) for different year\")\n",
    "print(f\"  - Proceed to gold layer integration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1646a7",
   "metadata": {},
   "source": [
    "## 9. Next Steps & Project Completion Status\n",
    "\n",
    "**Project Scope:** Chicago & Los Angeles × 2022, 2024 = 4 datasets\n",
    "\n",
    "**Workflow Reset Points:**\n",
    "\n",
    "**To process different city:**\n",
    "1. Update `CITY` variable in Section 2\n",
    "2. Rerun Sections 2-8\n",
    "3. Automatic directory creation\n",
    "\n",
    "**To process different year (same city):**\n",
    "1. Update `TARGET_YEAR` variable in Section 5\n",
    "2. Rerun Sections 5-8\n",
    "3. Uses existing filtered data\n",
    "\n",
    "---\n",
    "\n",
    "**Data Quality Notes:**\n",
    "\n",
    "All datasets show:\n",
    "- Complete date coverage for target year\n",
    "- Sufficient seasonal distribution (>10% minimum)\n",
    "- High review completeness (>99%)\n",
    "- Expected nulls in optional fields (host_about, neighborhood_overview, license)\n",
    "\n",
    "---\n",
    "\n",
    "**Upcoming Gold Layer Integration:**\n",
    "\n",
    "- **Multi-dataset analysis:** All processed silver datasets (TripAdvisor NYC, Yelp New Orleans, AirBnB LA/Chicago) will be explored for shared columns\n",
    "  \n",
    "- **Schema standardization:** Common fields (location, date, rating, text) will be unified across datasets\n",
    "  \n",
    "- **Data quality:** Null value handling, strategic imputation, and appropriate data type conversions\n",
    "  \n",
    "- **Analysis-ready format:** Final gold datasets optimized for sentiment analysis and tourism correlation modeling\n",
    "\n",
    "**Gold Processing Pipeline:**\n",
    "1. Load all silver datasets and analyze column overlap\n",
    "2. Standardize shared column names and formats\n",
    "3. Handle missing values with dataset-appropriate strategies\n",
    "4. Convert data types for analysis efficiency\n",
    "5. Create unified gold datasets for cross-platform analysis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tourism_data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
